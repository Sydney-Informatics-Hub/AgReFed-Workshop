{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1564fd4b",
   "metadata": {},
   "source": [
    "---\n",
    "title: Session 3 - Creating a custom settings file and running a harvest\n",
    "format: html\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388162a8",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "\n",
    "The Data Harvester enables researchers with reusable workflows for automatic data extraction from a range of data sources including spatial-temporal processing into useable formats. User provided data is auto-completed with a suitable set of spatial- and temporal-aligned covariates as a ready-made dataset for machine learning and agriculture models. In addition, all requested data layer maps are automatically extracted and aligned for a specific region and time period. \n",
    "\n",
    "The main workflow of the Harvester is as follows: \n",
    "\n",
    "1) Options and user settings (e.g., data layer selections, spatial coverage, temporal constraints, i/o directory names) are defined by the user in the notebook settings menu or can be loaded with a settings yaml file (e.g., settings/settings_v0.2_saved.yaml). All settings are also saved in a yaml file for reusability.\n",
    "2) The notebook imports settings and all Python modules that include functionality to download and extract data for each data source. After settings are read in, checked, and processed into valid data retrieval (API) queries, all selected data layers are sequentially downloaded and then processed into a clean dataframe table and co-registered raster maps. The entire workflow can be run either completely automatically or individually by selecting only certain process parts in the Notebook.\n",
    "\n",
    "Additional data sources can be best added by writing the API handlers and extraction functionalities as separate Python module, which are then imported by the Notebook. Currently the following data sources are supported by the following modules:\n",
    "\n",
    "- 'getdata_slga.py': Soil Data from Soil and Landscape Grid of Australia (SLGA)\n",
    "- 'getdata_landscape': Landscape data from Soil and Landscape Grid of Australia (SLGA)\n",
    "- 'getdata_silo.py': Climate Data from SILO\n",
    "- 'getdata_dem.py: 'National Digital Elevation Model (DEM) 1 Second plus Slope and Apect calculation\n",
    "- 'getdata_dea_nci.py: 'Digital Earth Australia's (DEA) Geoscience Earth Observations via NCI server\n",
    "- 'getdata_dea.py: 'Digital Earth Australia's (DEA) Geoscience Earth Observations via Open Web Service server provided by DEA\n",
    "- 'getdata_radiometric.py': Geoscience Australia National Geophysical Compilation Sub-collection Radiometrics\n",
    "- 'getdata_ee.py': Google Earth Engine API integration handler\n",
    "\n",
    "\n",
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6b7976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load general python libraries\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from os.path import exists\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Load geodata_harvester modules/functions/packages\n",
    "# See each python file for detailed options\n",
    "import geodata_harvester as gh\n",
    "\n",
    "from geodata_harvester import utils\n",
    "from geodata_harvester.arc2meter import calc_arc2meter\n",
    "from geodata_harvester.utils import init_logtable, update_logtable\n",
    "from geodata_harvester.widgets import harvesterwidgets as hw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f88c0e",
   "metadata": {},
   "source": [
    "## DIY settings configuration file\n",
    "\n",
    "Let's start with loading all user settings and options as specified in the settings file. For this example we provide a template file `data/settings_session1.yaml`. You can use the default settings in this file. \n",
    "Or you may changed the file directly, or point to a new file.\n",
    "Or override any of the defaults throughout this notebook.\n",
    "This is the core piece of the Data Harvester that makes the data collection reproduceable. You could give the settings file to someone else and they will end up with the same data collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build your own settings file with interactive widgets\n",
    "tab_nest, w_settings, names_settings, w_load = hw.gen_maintab()\n",
    "display(tab_nest) \n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54719eb4",
   "metadata": {},
   "source": [
    "When we have finished either loading in the settings file or choosing custom options we can validate the settings with the below code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "if w_load.value == None:\n",
    "    dict_settings = hw.eval_widgets(w_settings, names_settings)\n",
    "    # Convert settings from dictionary to SimpleNamespace (so all settings names available as settings.xxxname)\n",
    "    settings = SimpleNamespace(**dict_settings)\n",
    "    # Check if output path exists, if not create it:\n",
    "    os.makedirs(settings.outpath, exist_ok=True) \n",
    "    # Save settings to yaml file:\n",
    "    fname_settings = os.path.join(settings.outpath, 'settings_saved.yaml')\n",
    "    hw.save_dict_settings(dict_settings, fname_settings)\n",
    "else:\n",
    "    print(f'Settings loaded from {w_load.value}')\n",
    "    settings = hw.load_settings(w_load.value)\n",
    "# Print settings\n",
    "hw.print_settings(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbad12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the automated people you can also run something like...\n",
    "# load_settingsfilename = 'data/settings_session1.yaml'\n",
    "# settings = hw.load_settings(load_settingsfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d6caa2",
   "metadata": {},
   "source": [
    "## Setup dataset of interest\n",
    "\n",
    "Here we are reading in the point locations for which we want to extract data. A custom bounding box for which to extract raster data can be set in the settings file. If no bounding box provided, rasters are extracted for the region given by the point location extent plus an additional padding of 0.05 deg in Lat/Long (see code below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef8ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the dataset defining our location of interest as a geopandas dataframe\n",
    "gdfpoints = gpd.read_file(settings.infile)\n",
    "\n",
    "# Assing the data to well-named variables\n",
    "lngs = gdfpoints[settings.colname_lng].astype(float)\n",
    "lats = gdfpoints[settings.colname_lat].astype(float)\n",
    "\n",
    "# Check the data looks reasonable\n",
    "gdfpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf46b8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Info: Selected bounding box: {settings.target_bbox}')\n",
    "\n",
    "# Estimate resolution in meters:\n",
    "lat_center = (settings.target_bbox[1]+settings.target_bbox[3])/2\n",
    "xres_meters, yres_meters = calc_arc2meter(settings.target_res, lat_center)\n",
    "print(f'Info: {settings.target_res} arcsec resolution corresponds to {xres_meters:.1f}m x {yres_meters:.1f}m in x,y direction respectively (at Latitude: {lat_center:.2f}).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf07fc29",
   "metadata": {},
   "source": [
    "## Download and process data from API sources\n",
    "\n",
    "From here we automatically download and process sequentially a range of data sources as specified in the settings file (see next subsections: SLGA, SILO, DEA, DEM). Note that you may retrieve info and parameter input options for any function easily by running a function/method with a preceeding '?', e.g:\n",
    "```\n",
    "?getdata_slga.get_slga_layers\n",
    "?utils\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda8a279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a dataframe for logging all data output names and layer titles.\n",
    "# Note that the log table is later updated with update_logtable(), \n",
    "# which also instantly saves a copy of the table of the current status.\n",
    "df_log = init_logtable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87224156",
   "metadata": {},
   "source": [
    "### SLGA Download\n",
    "\n",
    "Here we download all requested data layers from the Soil and Landscape Grid of Australia (SLGA) for the given bounding box. Note that for this example we select the top soil (0 - 5cm) only. Optionally other layers and depths including confidence intervals can be extracted as well; for more details and options see getdata_slga.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ffb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can set the input options for each function call, and additional parameters may be set\n",
    "# too. Check the documentation of each function for full list of options.\n",
    "depth_min, depth_max = gh.getdata_slga.identifier2depthbounds(list(settings.target_sources['SLGA'].values())[0])\n",
    "slga_layernames = list(settings.target_sources['SLGA'].keys())\n",
    "\n",
    "fnames_out_slga = gh.getdata_slga.get_slga_layers(\n",
    "    slga_layernames, \n",
    "    settings.target_bbox, \n",
    "    settings.outpath, \n",
    "    depth_min = depth_min, \n",
    "    depth_max= depth_max, \n",
    "    get_ci = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daa38b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add download info to log dataframe\n",
    "df_log = update_logtable(\n",
    "    df_log, \n",
    "    fnames_out_slga, \n",
    "    slga_layernames, \n",
    "    'SLGA', \n",
    "    settings, \n",
    "    layertitles = [], loginfos = 'downloaded')\n",
    "df_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21115e5",
   "metadata": {},
   "source": [
    "### SILO Download\n",
    "\n",
    "Here we download climate data layers from SILO and extract raster for the given bounding box and year.\n",
    "For more details see getdata_silo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2719dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each data-source must be handled differently (as the data is stored in different ways)\n",
    "# Here we must get each layer, one by one. The simplest way is to loop through them.\n",
    "# Get data for each layer\n",
    "outpath = settings.outpath+'_silo'\n",
    "silo_layernames = list(settings.target_sources['SILO'].keys())\n",
    "# run the download\n",
    "fnames_out_silo = gh.getdata_silo.get_SILO_layers(\n",
    "    silo_layernames, \n",
    "    settings.date_min, \n",
    "    settings.date_max,\n",
    "    outpath, \n",
    "    bbox = settings.target_bbox, \n",
    "    format_out = 'tif')\n",
    "\n",
    "# Add download info to log dataframe\n",
    "# TBD need to be tested for multiple years and not only one\n",
    "if len(fnames_out_silo) > len(silo_layernames):\n",
    "    # TBD Temporary solution for multiple years:\n",
    "    nyears = int(len(fnames_out_silo)/len(silo_layernames))\n",
    "    silo_layernames = silo_layernames * nyears\n",
    "df_log = update_logtable(df_log, fnames_out_silo, silo_layernames, 'SILO', settings, layertitles = [], loginfos = 'downloaded')\n",
    "df_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a8a6f",
   "metadata": {},
   "source": [
    "### DEA Download\n",
    "\n",
    "Here we download satellite data from Digital Earth Australia (DEA) within the given bounding box and for all available image capture dates that are available within the specified year(s). For more details see getdata_dea.py or getdata_dea_nci\n",
    ".py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dfcaf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dea_layernames = settings.target_sources['DEA']\n",
    "\n",
    "# These are multiple files, so we put them in a subdirectory to make subsequent processing easier.\n",
    "outpath_dea = os.path.join(settings.outpath,'mvp_dea')\n",
    "\n",
    "outfnames = gh.getdata_dea.get_dea_layers_daterange(\n",
    "    dea_layernames, \n",
    "    settings.date_min,\n",
    "    settings.date_max,\n",
    "    settings.target_bbox, \n",
    "    settings.target_res, \n",
    "    outpath_dea, \n",
    "    crs = 'EPSG:4326', \n",
    "    format_out = 'GeoTIFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf143d7",
   "metadata": {},
   "source": [
    "#### DEA Processing\n",
    "\n",
    "This aggregates all images for the given year(s) and gnerates a combined image, here for example for the mean and 5th and 95th percentile each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872fa75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = []\n",
    "for layername in dea_layernames:\n",
    "    s = sum(layername in s for s in outfnames)\n",
    "    l = [layername]*s\n",
    "    layer_list.append(l)\n",
    "\n",
    "layer_list  = sum(layer_list, [])\n",
    "\n",
    "layer_titles = [os.path.splitext(x)[0].split('/')[-1] for x in outfnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abb9231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extracted data info to log table\n",
    "df_log = update_logtable(\n",
    "    df_log, \n",
    "    outfnames, \n",
    "    layer_titles, \n",
    "    'DEA', \n",
    "    settings, \n",
    "    layertitles = layer_titles, \n",
    "    loginfos = 'processed',force=True)\n",
    "#print(df_log.layertitle)\n",
    "df_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bbeeca",
   "metadata": {},
   "source": [
    "### DEM Download\n",
    "\n",
    "Here we download and extract the National Digital Elevation Model (DEM), and also generate slope and aspect rasters from the extracted DEM. \n",
    "For more details see getdata_dem.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe99242",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = os.path.join(settings.outpath, \"mvp_dem\")\n",
    "dem_layernames = settings.target_sources['DEM']\n",
    "outfnames = gh.getdata_dem.get_dem_layers(dem_layernames, outpath, settings.target_bbox, settings.target_res)\n",
    "\n",
    "# Add extracted data to log dataframe\n",
    "df_log = update_logtable(\n",
    "    df_log, \n",
    "    outfnames, \n",
    "    dem_layernames, \n",
    "    'DEM', \n",
    "    settings, \n",
    "    layertitles = dem_layernames,\n",
    "    loginfos = 'downloaded')\n",
    "df_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c284ec0a",
   "metadata": {},
   "source": [
    "### Landscape\n",
    "\n",
    "Download landscape data from Soil and Landscape Grid of Australia (SLGA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6099f901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download landscape data\n",
    "layernames = settings.target_sources['Landscape']\n",
    "layertitles = ['Landscape_' + layername for layername in layernames]\n",
    "\n",
    "outfnames = gh.getdata_landscape.get_landscape_layers(\n",
    "    layernames, \n",
    "    settings.target_bbox, \n",
    "    settings.outpath, \n",
    "    resolution = settings.target_res)\n",
    "\n",
    "# Add extracted data to log dataframe\n",
    "df_log = update_logtable(\n",
    "    df_log, outfnames, \n",
    "    layernames, \n",
    "    'Landscape', \n",
    "    settings, \n",
    "    layertitles = layertitles,\n",
    "    loginfos = 'downloaded')\n",
    "df_log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ceb804",
   "metadata": {},
   "source": [
    "### Radiometrics\n",
    "\n",
    "Download maps of Geoscience Australia National Geophysical Compilation Sub-collection Radiometrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8095f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download radiometrics\n",
    "layernames = settings.target_sources['Radiometric']\n",
    "\n",
    "outfnames = gh.getdata_radiometric.get_radiometric_layers(\n",
    "    settings.outpath, \n",
    "    layernames, \n",
    "    bbox = settings.target_bbox, \n",
    "    resolution=settings.target_res)\n",
    "\n",
    " # Add extracted data to log dataframe\n",
    "df_log = update_logtable(\n",
    "    df_log, outfnames, \n",
    "    layernames, \n",
    "    'Radiometric', \n",
    "    settings, \n",
    "    layertitles = layernames,\n",
    "    loginfos = 'downloaded')\n",
    "df_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdddc74",
   "metadata": {},
   "source": [
    "## Google Earth Engine\n",
    "\n",
    "To connect to Google Earth Engine, you must already have access to the Google\n",
    "Earth Engine API. You can request for access [by clicking\n",
    "here](https://earthengine.google.com/signup/). Once you are authorised, connect\n",
    "to the API using `initialise()`. A web browser may be invoked to complete the\n",
    "process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ee9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the Google Earth Engine API\n",
    "gh.getdata_ee.initialise()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc07c71",
   "metadata": {},
   "source": [
    "### Collect and preprocess Earth Engine Data\n",
    "\n",
    "Use `collect()` to define the data object and `preprocess()` to perform server-side\n",
    "data processing which includes cloud and shadow masking, image reduction and\n",
    "calculation of spectral indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3297ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gee = settings.target_sources[\"GEE\"]\n",
    "\n",
    "# Define the collection, area of interest and date range\n",
    "img = gh.getdata_ee.collect(collection=gee['preprocess']['collection'],\n",
    "              coords=settings.target_bbox,\n",
    "              date=gee[\"preprocess\"][\"date\"], \n",
    "              end_date = gee[\"preprocess\"][\"end_date\"])\n",
    "\n",
    "# Perform cloud maskng, reduction, and calculate one or more spectral indices\n",
    "img.preprocess(mask_clouds=gee[\"preprocess\"][\"mask_clouds\"], \n",
    "               reduce=gee[\"preprocess\"][\"reduce\"], \n",
    "               spectral=gee[\"preprocess\"][\"spectral\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f6cce",
   "metadata": {},
   "source": [
    "### Download\n",
    "\n",
    "Download the data using `download()` and add to `df_log`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3fa371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downoad data\n",
    "img.download(bands=gee[\"download\"][\"bands\"],\n",
    "             scale=gee[\"download\"][\"scale\"],\n",
    "             outpath=settings.outpath,\n",
    "             out_format=gee[\"download\"][\"format\"])\n",
    "\n",
    "\n",
    "# Add to log dataframe\n",
    "outfnames = [settings.outpath + img.filenames]\n",
    "layernames = [Path(img.filenames).resolve().stem]\n",
    "\n",
    "df_log = update_logtable(\n",
    "    df_log,\n",
    "    outfnames,\n",
    "    layernames,\n",
    "    \"GEE\",\n",
    "    settings,\n",
    "    layertitles=[],\n",
    "    agfunctions=img.reduce,\n",
    "    loginfos=\"downloaded\",\n",
    "    )\n",
    "\n",
    "df_log # preview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca05c9",
   "metadata": {},
   "source": [
    "## Save the final log or start from here to re-load it in.\n",
    "We have now completed the data download section. You may add additional downlods and processing steps to your log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428ded2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out (or load in) the log file.\n",
    "logfile = settings.outpath+'log.csv'\n",
    "if exists(logfile):\n",
    "    print(logfile, \"exists! Do you want to read it in?\\n\")\n",
    "    user_input = input(\"(y)es / (n)o / (a)bort ? Ansering 'no' will overwrite the current file.\\n\")\n",
    "    if user_input=='y':\n",
    "        df_log = pd.read_csv(settings.outpath+'log.csv')\n",
    "    elif user_input =='n': \n",
    "        df_log.to_csv(settings.outpath+'log.csv',index=False)\n",
    "        print(logfile, \"saved!\")\n",
    "    else:\n",
    "        print(\"Cancelling read/write for log file.\\nFigure out what you want to do and please try again.\")\n",
    "else:\n",
    "    print(\"No log file found. Saving to\", settings.outpath+'log.csv')\n",
    "    df_log.to_csv(settings.outpath+'log.csv',index=False)\n",
    "\n",
    "df_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a03a1e",
   "metadata": {},
   "source": [
    "## Points extraction from downloaded/processed data\n",
    "\n",
    "By default point values of all processed layers in df_log are extracted given by the input locations. However, you can select also only certain layers (see in code). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506fff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all processed data\n",
    "df_sel = df_log.copy()\n",
    "\n",
    "# or select only the rasters of interest, for example:\n",
    "\"\"\"\n",
    "df_sel = df_log[df_log['layername'].isin(['DEM','Slope',\n",
    "'landsat8_nbart_16day_channel0', \n",
    "'Organic_Carbon','Depth_of_Soil',\n",
    "'mean_temp','monthly_rain'])]\n",
    "\"\"\"\n",
    "\n",
    "rasters= df_sel['filename_out'].values.tolist()\n",
    "titles = df_sel['layertitle'].values.tolist()\n",
    "    \n",
    "# Extract datatable from rasters given input coordinates\n",
    "gdf = utils.raster_query(lngs,lats,rasters,titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba45d6d2",
   "metadata": {},
   "source": [
    "### Inspect result dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e246473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect either entire generated dataframe with \n",
    "# gdf\n",
    "# or only the first rows\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f62f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some general info about result table:\n",
    "gdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca670e",
   "metadata": {},
   "source": [
    "### Save the results table\n",
    "\n",
    "Finally, the result dataframe table is saved as a csv file, which can be used now to do some awesome ML.\n",
    "In addition the results are also saved as a geo-spatial referenced geopackage (.gpkg), which can be used again as input for further analysis or to inspect and overlay data on other layers and basemaps. The geopackage is a standard georeferenced file format and can be opened with any geo-spatial package or interactive software (e.g., QGIS, Esri ArcGIS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94bf31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results table to a csv \n",
    "gdf.to_csv(os.path.join(settings.outpath, \"results.csv\"), index = True, mode='w')\n",
    "\n",
    "# Save also as geopackage\n",
    "gdf.to_file(os.path.join(settings.outpath, \"results.gpkg\"), driver=\"GPKG\")\n",
    "# Note: The deprecated warning below is a bug in geopandas and will be fixed in their bext version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86caeb25",
   "metadata": {},
   "source": [
    "### Overview plot of all processed rasters \n",
    "\n",
    "This provides a quick overview to inspect all processed data layers with an overlay of the requested location points.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one of that datasets with the points on top\n",
    "utils.plot_rasters(rasters,lngs,lats,titles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
