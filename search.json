[
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Q: Who should attend these workshops?\nAnyone who is interested in extracting data from satellite imagery and/or national datasets. Although the training material has a strong emphasis on agricultural research, the techniques should apply anywhere remote sensing or satellite data are needed such as in ecology, forestry, fisheries, and environmental monitoring.\n\n\nQ: What are the pre-requisites?\nIf you are attending a Python Workshop, you should have a basic understanding of Python and understand how to use a command line interface (e.g. Terminal on Mac or Linux, or Command Prompt on Windows).\nIf you are attending an R Workshop, you should have a basic understanding of R and be comfortable with writing lines of code to perform actions. If you are not familiar with R, we recommend that you take a look at the R for Data Science book. Before attending a workshop, you should also install the RStudio IDE and the latest version of R.\n\n\nQ: What is the cost of attending these workshops?\nThese workshops are free to attend. However, we do ask that you register for the workshop so that we can plan accordingly. If you are unable to attend, please let us know so that we can open up your spot to someone else.\n\n\nQ: Why should I use the Data-Harvester over other tools?\nThe Data-Harvester is a good starting point for those who are new to remote sensing and satellite data. It provides a simple interface to access data and perform basic analysis. Importantly, users are not limited to the data and analysis provided by the Data-Harvester. We have made sure that data objects are in accessible formats which can be easily exported for use in other packages. For example, users can use the Data-Harvester to access data from the Google Earth Engine, and if they wish to perform additional transformations before exporting the data, they can export the Earth Engine object to the Earth Engine API in Python, or rgee if using R. or, they may download data using other products, but use the Data-Harvester to perform pixel reduction or temporal aggregation."
  },
  {
    "objectID": "Settings_Overview.html",
    "href": "Settings_Overview.html",
    "title": "Settings Overview",
    "section": "",
    "text": "The following documentation outlines the available settings for the Data Harvester\n\n\n\nYAML File Format\nJupyter Settings Widget\nSettings Validation\nInput and Output Settings\nSpatial and Temporal Settings\nData Selection Settings\n\n\n\n\nThe settings are specified by the user in a .yaml settings file (see e.g., settings/settings_v0.3.yaml). A YAML file is a Unicode based language and is designed for human interaction and to work well with modern programming languages, and is typically used for configuration settings and reusable workflows. YAML uses the .yaml extension (alternatively .yml) for its files. Its syntax is independent of a specific programming language.\nTemplates for the .yaml settings file are provided in the folder settings. More information about YAML Syntax can be found here.\n\n\n\nAlternatively, settings can be selected in the interactive widget of the Jupyter Notebook, which also automatically saves all settings for a run in a .yaml file as well. The interactive widgets are powered by ipywidgets and are currently supported for the Jupyter Notebooks. The widget also allows the user to load a saved .yaml file.\nNote for developers: To make changes to the functionality of the widgets (e.g, extending with new settings or options), please see the script harvesterwidgets.py in the folder widgets.\n\n\n\nThe settings file can be validated and checked for correct options (e.g. valid schema, data types, and data ranges) via the function validate in validate_settings.py, e.g.:\nfname_settings = 'settings_v0.3.yaml'\nimport validate_settings\nvalidate_settings.validate(fname_settings)\nNote for developers: Please update validate_settings.py and version if new data layers or options are added to the Data-Harvester.\n\n\n\nThe input file name is specified in infile and is a .csv file that and must include at least point coordinates. The Data Harvester will download new data for these coordinates and align with any given data in the input file. Th column names for the latitude and longitude coordinates are selected by the settings colname_lat and colname_lng, respectively.\nAll data results and images will be saved in the output directory as specified in the settings outpath.\nExample:\n#Input File:\ninfile: ../testdata/Pointdata_Llara.csv\n\n#Output Path:\noutpath: ../../dataresults/\n\n#Headername of Latitude in input file:\ncolname_lat: Lat\n\n#Headername of Longitude in input file:\ncolname_lng: Long\n\n\n\nThe spatial extent of the requested images can be given as bounding box list in the settings target_bbox, in the order: lng_min, lat_min, lng_max, lat_max (left, bottom, right, top corner of box). If no bounding box is provided, Data-Harvetser will automatically infer a padded bounding box based on the extent of the coordinates given in the input file.\nThe spatial resolution of the requested images is specified in target_res and given in arcsec (1 arcsec corresponds to roughly 30m on the Equator, please see arc2meter.pyfor calculating exact conversion of meter to arcsec and vice versa).\nThe years for the requested data is specified via target_dates and can be one specific year or a list of multiple years.\nTBD: - The temporal resolution specifies the length of the time (in days) for which data is aggregated. The date range will then be subdivided in n bins = maximum year - minimum year divided by temporal resolution - Spatial buffer\nExample:\n#Bounding Box as (lng_min, lat_min, lng_max, lat_max):\ntarget_bbox: ''\n\n#Select years:\ntarget_dates:\n  - 2021\n\n#Spatial Resolution [in arcsec]:\ntarget_res: 6.0\n\n#Temporal Resolution [in days, from 1 to 365 days], TBD\ntemp_res: 365\n\n\n\nThe requested layers are specified in the settings target_sources. The following data sources are currently supported:\n\n\nThese are pre-processed and national calibrated satellite image layers provided Digital Earth Australia (DEA) Geoscience Earth Observations. Multiple layers can be given as list in the settings. For more details see Data Overview DEA.\n\n\n\nThe DEM data is given by the National Digital Elevation Model 1 Second Hydrologically Enforced. Options are: ‘DEM’, ‘Slope’, and ‘Aspect’. For more info see Data Overview DEM.\n\n\n\nLandscape data can be retrieved from SLGA. For an overview of all available layers see Data Overview Landscape.\n\n\n\nFor an overview of the radiometric layer options see Data Overview Radiometric.\n\n\n\nSILO is containing continuous daily climate data for Australia. An overview of the available data layers is provided in Data Overview SILO.\nFor each requested SILO data layer, at least one temporal aggregation method has to be provided, which will be applied to aggregate climate data over the specified temporal range. The following aptions are available: ‘mean’, ‘median’, ‘sum’, ‘std’, ‘perc95’, ‘perc5’, ‘max’, ‘min’\n\n\n\nAn overview of the soil attributes is given in in Data Overview SLGA.\nEach soil attribute has six depth layers (plus their upper and lower confidence limits), with the following options:‘0-5cm’, ‘5-15cm’, ‘15-30cm’, ‘30-60cm’, ‘60-100cm’ and ‘100-200cm’.\n\n\n\nAn overview of the available Google Earth Engine (GEE) data is provided in Data Overview GEE\nDocumentation of options: TBD\nExample:\ntarget_sources:\n  #Satellite data from Digital Earth Australia\n  DEA:\n  - landsat_barest_earth\n\n  #National Digital Elevation Model (DEM) 1 Second\n  DEM:\n  - DEM\n  \n  #Landscape Data \n  Landscape:\n  - Slope\n  - Aspect\n  - Relief_300m\n\n  #Radiometric Data\n  Radiometric:\n  - radmap2019_grid_dose_terr_awags_rad_2019\n  - radmap2019_grid_dose_terr_filtered_awags_rad_2019\n\n  # SILO Climate Data\n  # temporal aggregation options: 'mean', 'median', 'sum', 'std', 'perc95', 'perc5', 'max', 'min'\n  SILO:\n    max_temp:\n    - Median\n    min_temp:\n    - Median\n    monthly_rain:\n    - Total\n\n  #Soil data from SLGA\n  SLGA:\n   Bulk_Density:\n    - 0-5cm\n   Clay:\n    - 0-5cm\n\n  #Satellite data layers from Google Earth Engine\n  GEE: \n    preprocess:\n\n      ### collection as defined in the Earth Engine Catalog\n      collection: LANDSAT/LC09/C02/T1_L2\n\n      #### if supplied, will use 'buffer' and 'bound'. else, will use bbox above\n      coords: [149.769345, -30.335861]\n\n      #### If date range is supplied, will use below. Else, will use `target_dates`\n      date: 2021-01-01\n      end_date: 2021-12-31\n\n      #### circular buffer in metres\n      buffer: null\n\n      #### convert buffer into square bounding box instead\n      bound: null\n\n      #### cloud masking option\n      mask_clouds: True\n\n      #### if null, will download all available images. Else, will reduce to single\n\n      #### composite image based on summary stat provided\n      reduce: median\n\n      #### spectral indices to calculate via Awesome Spectral Indices site\n      spectral:\n        - NDVI\n        - NDWI\n    aggregate:\n      \n      #### group data by period. Available: year, month, week\n      frequency: year \n      #### summarise group by method\n      method: mean  \n\n    download:\n      bands: \n        - NDVI\n        - SR_B2\n        - SR_B3\n        - SR_B4\n      scale: 100   # in metres\n      format: tif  # available: tif, png"
  },
  {
    "objectID": "yaml_config.html",
    "href": "yaml_config.html",
    "title": "The YAML configuration file",
    "section": "",
    "text": "Note\n\n\n\nThis section documents the YAML file format, but does not cover running a YAML file. For information on running the file, see the Introduction sections of the respective workshops\nYAML stands for YAML Ain’t Markup Language and is a human-readable data serialization format that is commonly used for configuration files. A YAML file is identified by its file extension, .yaml.\nIn the Data-Harvester, you can use YAML to configure data sources, filters and transformations to achieve minimal interaction with the command line.\nBelow is a simple example of a YAML configuration file that downloads two products from one API source using a bounding box that is estimated from the input file Llara.csv:\nUsing the configuration file is the simplest way to make your downloads reproducible as it allows you to run the same command multiple times without having to specify the same parameters each time.\nIn the future, all of the Data-Harvester code will be accessible through the YAML configuration file, but for now, there are limitations as new features are still being added and tested."
  },
  {
    "objectID": "yaml_config.html#infile",
    "href": "yaml_config.html#infile",
    "title": "The YAML configuration file",
    "section": "infile",
    "text": "infile\nString. Path to a file containing geospatial coordinate information. The file must contain two columns, one for latitude and one for longitude. The column names can be specified using the colname_lat and colname_lng parameters. The file must be a .csv file. A relative path can be used.\ninfile: Llara.csv"
  },
  {
    "objectID": "yaml_config.html#outpath",
    "href": "yaml_config.html#outpath",
    "title": "The YAML configuration file",
    "section": "outpath",
    "text": "outpath\nString. Path to the directory where the downloaded data will be saved. The directory will be created if it does not already exist. A relative path can be used.\noutpath: downloads/"
  },
  {
    "objectID": "yaml_config.html#colname_lat",
    "href": "yaml_config.html#colname_lat",
    "title": "The YAML configuration file",
    "section": "colname_lat",
    "text": "colname_lat\nString. Case-sensitive name of the column in the input file that contains latitude information. This parameter is only required if the input file is provided. Will be ignored if infile is null.\ncolname_lat: Lat"
  },
  {
    "objectID": "yaml_config.html#colname_lng",
    "href": "yaml_config.html#colname_lng",
    "title": "The YAML configuration file",
    "section": "colname_lng",
    "text": "colname_lng\nString. Case-sensitive name of the column in the input file that contains longitude information. This parameter is only required if the input file is provided. Will be ignored if infile is null."
  },
  {
    "objectID": "yaml_config.html#target_bbox",
    "href": "yaml_config.html#target_bbox",
    "title": "The YAML configuration file",
    "section": "target_bbox",
    "text": "target_bbox\nList of floats. Bounding box that defines the area of interest. The bounding box must be provided as a list of four numbers: [min_lat, min_lng, max_lat, max_lng]. If the input file is provided, the bounding box will be estimated from the coordinates in the input file.\n\nIf infile is not provided, the bounding box must be provided.\n\ninfile: null\ntarget_bbox: [149.769345, -30.335861, 149.949173, -30.206271]\n\nIf infile is provided, the bounding box can be provided. If the bounding box is not provided, the area of interested will be estimated from the coordinates in the input file.\n\ninfile: Llara.csv\ntarget_bbox: null"
  },
  {
    "objectID": "yaml_config.html#target_dates",
    "href": "yaml_config.html#target_dates",
    "title": "The YAML configuration file",
    "section": "target_dates",
    "text": "target_dates\nDate(s) that define the time period of interest. The dates must be provided as integers and in YYYY format."
  },
  {
    "objectID": "yaml_config.html#target_res",
    "href": "yaml_config.html#target_res",
    "title": "The YAML configuration file",
    "section": "target_res",
    "text": "target_res\nSpatial resolution of the data to be downloaded. The resolution must be provided as a float and is in arc-seconds."
  },
  {
    "objectID": "yaml_config.html#target_sources",
    "href": "yaml_config.html#target_sources",
    "title": "The YAML configuration file",
    "section": "target_sources",
    "text": "target_sources\nAPI sources to download data from. Available sources are:\n\nDEA\nDEM\nGEE\nLandscape\nSILO\nSLGA\nRadiometric\n\nEach API source has a list of products that can be downloaded. These product names can be obtained from the respective API websites, or from our  Data Overview. Google Earth Engine (GEE) datasets are updated on the Earth Engine Catalog.\nSILO (Scientific Information for Land Owners) and GEE have additional parameters that can be specified. These parameters are desribed below.\n\nSILO\n\n\nGEE"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data-Harvester Workshops",
    "section": "",
    "text": "Workshop information\n\n\n\nThe next AgReFed Data-Harvester Workshop in R will be run on Tue, 25 Oct, 2022, at 09:30am AEST (Sydney). Please see R Workshop for details.\n\nWorkshop starts in…\n Note: The above workshop is open to staff and students of The University of Sydney only. The next workshop, also in R, can be registered by anyone and is now open for registration. This workshop will be run on Thu, 03 Nov, 2022, at 01:30pm AEST (Sydney)."
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Data-Harvester Workshops",
    "section": "About",
    "text": "About\nUse the AgReFed Data-Harvester to access, process and download national (Australian) and global space/time data aimed at agricultural scientists, environmental scientists, ecologists and other researchers.\nIn the workshops, we will show you how to download and summarise geospatial data from a range of sources including:\n\n\nSoil and Landscape Grid of Australia (SLGA)\nSILO Climate Database\nDigital Elevation Model (DEM) of Australia\nDigital Earth Australia (DEA) Geoscience Earth Observations\nGSKY Data Server for DEA Geoscience Earth Observations\nGoogle Earth Engine\n\nIn addition, we will present some spatial and temporal aggregation, visualisation, and simple summary techniques.\n\nFor more information about the workshops, including who these workshops are made for, check the FAQ."
  },
  {
    "objectID": "index.html#trainers",
    "href": "index.html#trainers",
    "title": "Data-Harvester Workshops",
    "section": "Trainers",
    "text": "Trainers\n\nJanuar Harianto (R Workshop Trainer)\nNathaniel Butterworth (Python Workshop Trainer)\nSebastian Haan (support)\nHenry Lydecker (support)\nEden Zhang (support)\nDarya Vanichkina (support)\nThomas Bishop (support)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Us",
    "section": "",
    "text": "AgReFed is a cooperative of Data Provider Communities with a shared vision to enable Findable, Accessible, Interoperable and Reusable (FAIR) agricultural data to accelerate innovation and increase the profitability and sustainability of Australian agriculture, through the creation of a unifying federation for the sharing of agricultural data amongst like-minded Data Provider Communities.\nAgReFed is committed to pursuing a shared mission to unlock the potential of agricultural data from Australian research organisations, government, agricultural producers and other agricultural industry players by providing a data sharing platform enabling the use of FAIR data to increase the application of knowledge, accelerate innovation and improve decision making.\nAgReFed pursues this mission by:\n\nBringing together and aligning independent organisations to make strategic and technical decisions about data sharing\nProviding a governance and data stewardship framework for collective decision making\nProviding the infrastructure, tools, and resources for Data Provider Communities to develop the capacity to make agricultural data FAIR\nEnabling increasingly FAIR data from research organisations to be available for use by the broader agricultural research community\nIncorporating and enabling access to complementary data important to agricultural research\n\n\n\n\n\n\n\n\n\n\n\n The Sydney Informatics Hub (SIH) is a Core Research Facility of the University of Sydney. Core Research Facilities centralise essential research equipment and services that would otherwise be too expensive or impractical for individuals, Schools or Faculties to purchase and maintain.\nWe provide a wide range of research services to aid investigators, such as:\n\nTraining and workshops\nProject consulting and assistance with Statistics, Data Science, Research Engineering, Bioinformatics, Modeling/Simulation/Visualisation, High Performance Computing.\nResearch data management consulting and platform support.\n\nWe also aim to cultivate a data community, organising monthly Hacky Hours, outside training events (eg NVIDIA, Pawsey Center), and data/coding-related events. Look out for everything happening on our calendar or contact us at sih.info@sydney.edu.au to get some digital collaboration going."
  },
  {
    "objectID": "pydocs/Data_Harvest.html",
    "href": "pydocs/Data_Harvest.html",
    "title": "AgReFed Data-Harvester",
    "section": "",
    "text": "The Data Harvester enables researchers with reusable workflows for automatic data extraction from a range of data sources including spatial-temporal processing into useable formats. User provided data is auto-completed with a suitable set of spatial- and temporal-aligned covariates as a ready-made dataset for machine learning and agriculture models. In addition, all requested data layer maps are automatically extracted and aligned for a specific region and time period.\nThe main workflow of the Harvester is as follows: 1) Options and user settings (e.g., data layer selections, spatial coverage, temporal constraints, i/o directory names) are defined by the user in the notebook settings menu or can be loaded with a settings yaml file (e.g., settings/settings_v0.2_saved.yaml). All settings are also saved in a yaml file for reusability. 2) The notebook imports settings and all Python modules that include functionality to download and extract data for each data source. After settings are read in, checked, and processed into valid data retrieval (API) queries, all selected data layers are sequentially downloaded and then processed into a clean dataframe table and co-registered raster maps. The entire workflow can be run either completely automatically or individually by selecting only certain process parts in the Notebook.\nAdditional data sources can be best added by writing the API handlers and extraction functionalities as separate Python module, which are then imported by the Notebook. Currently the following data sources are supported by the following modules:\n\n‘getdata_slga.py’: Soil Data from Soil and Landscape Grid of Australia (SLGA)\n‘getdata_landscape’: Landscape data from Soil and Landscape Grid of Australia (SLGA)\n‘getdata_silo.py’: Climate Data from SILO\n’getdata_dem.py: ’National Digital Elevation Model (DEM) 1 Second plus Slope and Apect calculation\n’getdata_dea_nci.py: ’Digital Earth Australia’s (DEA) Geoscience Earth Observations via NCI server\n’getdata_dea.py: ’Digital Earth Australia’s (DEA) Geoscience Earth Observations via Open Web Service server provided by DEA\n‘getdata_radiometric.py’: Geoscience Australia National Geophysical Compilation Sub-collection Radiometrics\n\nFor more details. please see README and the Data Overview page.\nThis notebook is part of the Data Harvester project developed for the Agricultural Research Federation (AgReFed).\nCopyright 2022 Sydney Informatics Hub (SIH), The University of Sydney\n\n#Load general python libraries\nimport geopandas as gpd\nimport pandas as pd\nimport os\nfrom os.path import exists\nimport time\nfrom datetime import datetime\nfrom types import SimpleNamespace \nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Load local modules/functions/packages\n# See each python file for detailed options\nimport getdata_silo \nimport getdata_slga \nimport getdata_dea\nimport getdata_dem\nimport getdata_radiometric\nimport getdata_landscape\nimport utils\nfrom utils import init_logtable, update_logtable\nfrom arc2meter import calc_arc2meter\n\n\n# This cell is tagged with \"parameters\" if notebook is run with papermill command line arguments (leave blank)\nload_settingsfilename = ''\n\n\n\n\n#NEW: For importing custom settings widgets\nfrom widgets import harvesterwidgets as hw\ntab_nest, w_settings, names_settings, w_load = hw.gen_maintab()\ndisplay(tab_nest) \n#Note: the display screen may take a couple of seconds more after loading\ntime.sleep(8)"
  },
  {
    "objectID": "pydocs/Data_Harvest.html#import-settings",
    "href": "pydocs/Data_Harvest.html#import-settings",
    "title": "AgReFed Data-Harvester",
    "section": "Import settings",
    "text": "Import settings\nLet’s start with loading all user settings and options as specified in the settings file. For this example we provide a template file settings/settings_v0.1_default.yaml. You can comfortable use the default settings in this file. Or you may changed the file directly, or point to a new file. Or override any of the defaults throughout this notebook.\n\n#For recording time:\nstart_time = datetime.now()\n\nif load_settingsfilename != '':\n    # load settings fromm file given by command line argument\n    print(f'Automatinc loading settings from {load_settingsfilename}')\n    settings = hw.load_settings(load_settingsfilename)\nelif w_load.value == None:\n    # if no settings file selected, convert widgets inputs above to settings\n    dict_settings = hw.eval_widgets(w_settings, names_settings)\n    # Convert settings from dictionary to SimpleNamespace (so all settings names available as settings.xxxname)\n    settings = SimpleNamespace(**dict_settings)\n    # Check if output path exists, if not create it:\n    os.makedirs(settings.outpath, exist_ok=True) \n    # Save settings to yaml file:\n    hw.save_dict_settings(dict_settings, os.path.join(settings.outpath, 'settings_saved.yaml'))\nelse:\n    print(f'Settings loaded from {w_load.value}')\n    settings = hw.load_settings(w_load.value)\nhw.print_settings(settings)\n\nSettings saved to file ../dataresults_testnotebook/settings_saved.yaml\nSettings loaded:\n----------------\nsettings.infile : /Users/seb/CTDS/Projects/AgReFed/Harvester/code/AgReFed-DataHarvester/testdata/Pointdata_Llara.csv\nsettings.outpath : ../dataresults_testnotebook/\nsettings.colname_lng : Long\nsettings.colname_lat : Lat\nsettings.target_bbox : \nsettings.target_res : 12.0\nsettings.target_dates : (2021,)\nsettings.temp_res : 365\nsettings.target_sources:\n   'SLGA': {'Bulk_Density': ['0-5cm']}\n   'SILO': {'monthly_rain': ['Total']}\n   'DEA': ['landsat_barest_earth']\n   'DEM': ['DEM', 'Slope', 'Aspect']\n   'Radiometric': ['radmap2019_grid_dose_terr_filtered_awags_rad_2019']\n   'Landscape': ['Relief_300m']"
  },
  {
    "objectID": "pydocs/Data_Harvest.html#setup-dataset-of-interest",
    "href": "pydocs/Data_Harvest.html#setup-dataset-of-interest",
    "title": "AgReFed Data-Harvester",
    "section": "Setup dataset of interest",
    "text": "Setup dataset of interest\nHere we are reading in the point locations for which we want to extract data. A custom bounding box for which to extract raster data can be set in the settings file. If no bounding box provided, rasters are extracted for the region given by the point location extent plus an additional padding of 0.05 deg in Lat/Long (see code below).\n\n# Load in the dataset defining our location of interest as a geopandas dataframe\ngdfpoints = gpd.read_file(settings.infile)\n\n# This particular dataset contains duplicate point locations at different depths.\n# We can take advantage of the Notebook environment to make small manipulations\n# to pull out just the data we need, i.e:\ngdfpoints=gdfpoints.loc[gdfpoints['depth'] == \"0-5 cm\"]\n\n# Assing the data to well-named variables\nlongs = gdfpoints[settings.colname_lng].astype(float)\nlats = gdfpoints[settings.colname_lat].astype(float)\n\n\n# Check the data looks reasonable\ngdfpoints.head()\n\n\n\n\n\n  \n    \n      \n      field_1\n      Lat\n      Long\n      Easting\n      Northing\n      depth\n      geometry\n    \n  \n  \n    \n      0\n      0\n      -30.264663\n      149.85268\n      774457.572546495\n      6648441.94497259\n      0-5 cm\n      None\n    \n    \n      5\n      5\n      -30.265302\n      149.884838\n      777550.996253435\n      6648292.91822913\n      0-5 cm\n      None\n    \n    \n      9\n      9\n      -30.265302\n      149.884838\n      777550.996253435\n      6648292.91822913\n      0-5 cm\n      None\n    \n    \n      14\n      14\n      -30.278542\n      149.838791\n      773082.294868699\n      6646936.5315563\n      0-5 cm\n      None\n    \n    \n      19\n      19\n      -30.275437\n      149.830843\n      772325.998393026\n      6647299.91001948\n      0-5 cm\n      None\n    \n  \n\n\n\n\n\n# Use padding area of interest +/- 0.05 deg if no bbox provided. \nif (settings.target_bbox == None) | (settings.target_bbox == 'None') | (settings.target_bbox == ''):\n    settings.target_bbox = (min(longs)-0.05,min(lats)-0.05,max(longs)+0.05,max(lats)+0.05)\nprint(f'Info: Selected bounding box: {settings.target_bbox}')\n\n# Estimate resolution in meters:\nlat_center = (settings.target_bbox[1]+settings.target_bbox[3])/2\nxres_meters, yres_meters = calc_arc2meter(settings.target_res, lat_center)\nprint(f'Info: {settings.target_res} arcsec resolution corresponds to {xres_meters:.1f}m x {yres_meters:.1f}m in x,y direction respectively (at Latitude: {lat_center:.2f}).')\n\nInfo: Selected bounding box: (149.769345, -30.335861, 149.949173, -30.206271)\nInfo: 6.0 arcsec resolution corresponds to 160.2m x 185.2m in x,y direction respectively (at Latitude: -30.27)."
  },
  {
    "objectID": "pydocs/Data_Harvest.html#download-and-process-data-from-api-sources",
    "href": "pydocs/Data_Harvest.html#download-and-process-data-from-api-sources",
    "title": "AgReFed Data-Harvester",
    "section": "Download and process data from API sources",
    "text": "Download and process data from API sources\nFrom here we automatically download and process sequentially a range of data sources as specified in the settings file (see next subsections: SLGA, SILO, DEA, DEM). Note that you may retrieve info and parameter input options for any function easily by running a function/method with a preceeding ‘?’, e.g:\n?getdata_slga.get_slga_layers\n?utils\n\n# Initiate a dataframe for logging all data output names and layer titles.\n# Note that the log table is later updated with update_logtable(), which also instantly saves a copy of the table of the current status.\ndf_log = init_logtable()\n\n\nSLGA Download\nHere we download all requested data layers from the Soil and Landscape Grid of Australia (SLGA) for the given bounding box. Note that for this example we select the top soil (0 - 5cm) only. Optionally other layers and depths including confidence intervals can be extracted as well; for more details and options see getdata_slga.py.\n\n# We can set the input options for each function call, and additional parameters may be set\n# too. Check the documentation of each function for full list of options.\ndepth_min, depth_max = getdata_slga.identifier2depthbounds(list(settings.target_sources['SLGA'].values())[0])\nslga_layernames = list(settings.target_sources['SLGA'].keys())\nfnames_out_slga = getdata_slga.get_slga_layers(\n    slga_layernames, \n    settings.target_bbox, \n    settings.outpath, \n    depth_min = depth_min, \n    depth_max= depth_max, \n    get_ci = True)\n\nDownloading Bulk_Density...\nSLGA_Bulk_Density_0-5cm downloaded. Saved to:  ../../dataresults/SLGA_Bulk_Density_0-5cm.tif\nDownloading confidence intervals for Bulk_Density...\nSLGA_Bulk_Density_0-5cm CIs downloaded.\nDownloading Clay...\n../../dataresults/SLGA_Clay_0-5cm.tif already exists\nSLGA_Clay_0-5cm downloaded. Saved to:  ../../dataresults/SLGA_Clay_0-5cm.tif\nDownloading confidence intervals for Clay...\n../../dataresults/SLGA_Clay_0-5cm_5percentile.tif already exists\n../../dataresults/SLGA_Clay_0-5cm_95percentile.tif already exists\nSLGA_Clay_0-5cm CIs downloaded.\nSLGA Download complete.\n\n\n\n# Add download info to log dataframe\ndf_log = update_logtable(df_log, fnames_out_slga, slga_layernames, 'SLGA', settings, layertitles = [], loginfos = 'downloaded')\ndf_log\n\n\n\n\n\n  \n    \n      \n      layername\n      agfunction\n      dataset\n      layertitle\n      filename_out\n      loginfo\n    \n  \n  \n    \n      0\n      Bulk_Density\n      0-5cm\n      SLGA\n      Bulk_Density_0-5cm\n      ../../dataresults/SLGA_Bulk_Density_0-5cm.tif\n      downloaded\n    \n    \n      1\n      Clay\n      0-5cm\n      SLGA\n      Clay_0-5cm\n      ../../dataresults/SLGA_Clay_0-5cm.tif\n      downloaded\n    \n  \n\n\n\n\n\n\nSILO Download\nHere we download climate data layers from SILO and extract raster for the given bounding box and year. For more details see getdata_silo.py\n\n# Each data-source must be handled differently (as the data is stored in different ways)\n# Here we must get each layer, one by one. The simplest way is to loop through them.\n# Get data for each layer\nfnames_out_silo = []\nsilo_layernames = list(settings.target_sources['SILO'].keys())\nfor layername in silo_layernames:\n    # define output file name\n    outpath = settings.outpath+'mvp_'+layername+'_silo'\n    # run the download\n    fnames_out = getdata_silo.get_SILO_raster(\n        layername, \n        settings.target_dates, \n        outpath, \n        bbox = settings.target_bbox, \n        format_out = 'tif', \n        delete_temp= False)\n    #Save the layer name\n    fnames_out_silo += fnames_out\n\n# Add download info to log dataframe\ndf_log = update_logtable(df_log, fnames_out_silo, silo_layernames, 'SILO', settings, layertitles = [], loginfos = 'downloaded')\ndf_log\n\nDownloading data for year 2021 from https://s3-ap-southeast-2.amazonaws.com/silo-open-data/Official/annual/monthly_rain/2021.monthly_rain.nc ...\n../../dataresults/mvp_monthly_rain_silo/2021.monthly_rain.nc already exists\nSaved monthly_rain for year 2021 as geotif: \n../../dataresults/mvp_monthly_rain_silo/monthly_rain_2021_cropped.tif\nDownloading data for year 2021 from https://s3-ap-southeast-2.amazonaws.com/silo-open-data/Official/annual/max_temp/2021.max_temp.nc ...\n../../dataresults/mvp_max_temp_silo/2021.max_temp.nc already exists\nSaved max_temp for year 2021 as geotif: \n../../dataresults/mvp_max_temp_silo/max_temp_2021_cropped.tif\nDownloading data for year 2021 from https://s3-ap-southeast-2.amazonaws.com/silo-open-data/Official/annual/min_temp/2021.min_temp.nc ...\n../../dataresults/mvp_min_temp_silo/2021.min_temp.nc already exists\nSaved min_temp for year 2021 as geotif: \n../../dataresults/mvp_min_temp_silo/min_temp_2021_cropped.tif\n\n\n\n\n\n\n  \n    \n      \n      layername\n      agfunction\n      dataset\n      layertitle\n      filename_out\n      loginfo\n    \n  \n  \n    \n      0\n      Bulk_Density\n      0-5cm\n      SLGA\n      Bulk_Density_0-5cm\n      ../../dataresults/SLGA_Bulk_Density_0-5cm.tif\n      downloaded\n    \n    \n      1\n      Clay\n      0-5cm\n      SLGA\n      Clay_0-5cm\n      ../../dataresults/SLGA_Clay_0-5cm.tif\n      downloaded\n    \n    \n      2\n      monthly_rain\n      Total\n      SILO\n      monthly_rain_Total\n      ../../dataresults/mvp_monthly_rain_silo/monthl...\n      downloaded\n    \n    \n      3\n      max_temp\n      Median\n      SILO\n      max_temp_Median\n      ../../dataresults/mvp_max_temp_silo/max_temp_2...\n      downloaded\n    \n    \n      4\n      min_temp\n      Median\n      SILO\n      min_temp_Median\n      ../../dataresults/mvp_min_temp_silo/min_temp_2...\n      downloaded\n    \n  \n\n\n\n\n\nSILO Processing\nThis is an example for further processing of the extracted SILO data. Here we are interested in generating a mean temperature raster given the already extracted min and max temperature rasters.\n\n#Gere we want to immediately perform some data processing on the SILO layers.\n\n# Sub select whatever files we want to aggregate, from the log file\nfile_list = df_log[df_log['layername'].isin(['min_temp','max_temp'])].filename_out.to_list()\n\nif len(file_list) == 2:\n    # Both have a recommendation of running mean, so lets set that\n    agg = ['mean']\n\n    # Set an output filename if wanted\n    outfile = settings.outpath+'silo_temp_2019_ag'\n\n    # And run the processing\n    outfname_agg = utils.aggregate_rasters(\n        file_list=file_list,\n        outfile=outfile, \n        data_dir=None,\n        agg=agg)\n        \n    # Add processed info to log dataframe\n    df_log = update_logtable(df_log, [outfname_agg[0]], ['mean_temp'], 'SILO', settings, layertitles = ['mean_temp'], agfunctions = ['mean'], loginfos = 'processed')\n    df_log\n\nFinding ['mean']  out of possible ['mean', 'median', 'sum', 'perc95', 'perc5']\nmean of filelist saved in:  ../../dataresults/silo_temp_2019_ag_mean.tif\n\n\n\n\n\nDEA Download\nHere we download satellite data from Digital Earth Australia (DEA) within the given bounding box and for all available image capture dates that are available within the specified year(s). For more details see getdata_dea.py or getdata_dea_nci .py\n\ndea_layernames = settings.target_sources['DEA']\n\n# These are multiple files, so we put them in a subdirectory to make subsequent processing easier.\noutpath_dea = os.path.join(settings.outpath,'mvp_dea')\n\noutfnames = getdata_dea.get_dea_layers(\n    dea_layernames, \n    settings.target_dates, \n    settings.target_bbox, \n    settings.target_res, \n    outpath_dea, \n    crs = 'EPSG:4326', \n    format_out = 'GeoTIFF')\n\nNumber of images for 2021 found: 0\nNo dates found for year 2021. Trying to download without date.\nDownloading landsat_barest_earth for date None ...\nlandsat_barest_earth for date None downloaded\nAll layers downloads completed and saved in directory ../../dataresults/mvp_dea.\n\n\n\nDEA Processing\nThis aggregates all images for the given year(s) and gnerates a combined image, here for example for the mean and 5th and 95th percentile each.\n\n# Process DEA data over time aggregates\noutfname_list, channel_list, agg_list = utils.aggregate_multiband(\n    data_dir = outpath_dea,\n    outfile = settings.outpath+\"mvp_dea\",\n    agg = ['mean','perc95','perc5'],\n    file_list = None)\n\nFinding ['mean', 'perc95', 'perc5']  out of possible ['mean', 'median', 'sum', 'perc95', 'perc5']\nReading all *.tif files in:  ../../dataresults/mvp_dea\nmean of filelist saved in:  ../../dataresults/mvp_dea_mean_channel_0.tif\nperc95 of filelist saved in:  ../../dataresults/mvp_dea_perc95_channel_0.tif\nperc5 of filelist saved in:  ../../dataresults/mvp_dea_perc5_channel_0.tif\nmean of filelist saved in:  ../../dataresults/mvp_dea_mean_channel_1.tif\nperc95 of filelist saved in:  ../../dataresults/mvp_dea_perc95_channel_1.tif\nperc5 of filelist saved in:  ../../dataresults/mvp_dea_perc5_channel_1.tif\nmean of filelist saved in:  ../../dataresults/mvp_dea_mean_channel_2.tif\nperc95 of filelist saved in:  ../../dataresults/mvp_dea_perc95_channel_2.tif\nperc5 of filelist saved in:  ../../dataresults/mvp_dea_perc5_channel_2.tif\n\n\n\n# Add extracted data info to log table\nlayernames = [layername + '_channel' + channel_list[i] for i in range(len(channel_list))]\ndf_log = update_logtable(df_log, outfname_list, layernames, 'DEA', settings, agfunctions = agg_list, loginfos = 'processed')\n#print(df_log.layertitle)\ndf_log\n\n\n\n\n\n  \n    \n      \n      layername\n      agfunction\n      dataset\n      layertitle\n      filename_out\n      loginfo\n    \n  \n  \n    \n      0\n      Bulk_Density\n      0-5cm\n      SLGA\n      Bulk_Density_0-5cm\n      ../../dataresults/SLGA_Bulk_Density_0-5cm.tif\n      downloaded\n    \n    \n      1\n      Clay\n      0-5cm\n      SLGA\n      Clay_0-5cm\n      ../../dataresults/SLGA_Clay_0-5cm.tif\n      downloaded\n    \n    \n      2\n      monthly_rain\n      Total\n      SILO\n      monthly_rain_Total\n      ../../dataresults/mvp_monthly_rain_silo/monthl...\n      downloaded\n    \n    \n      3\n      max_temp\n      Median\n      SILO\n      max_temp_Median\n      ../../dataresults/mvp_max_temp_silo/max_temp_2...\n      downloaded\n    \n    \n      4\n      min_temp\n      Median\n      SILO\n      min_temp_Median\n      ../../dataresults/mvp_min_temp_silo/min_temp_2...\n      downloaded\n    \n    \n      5\n      mean_temp\n      mean\n      SILO\n      mean_temp\n      ../../dataresults/silo_temp_2019_ag_mean.tif\n      processed\n    \n    \n      6\n      min_temp_channel0\n      mean\n      DEA\n      min_temp_channel0_mean\n      ../../dataresults/mvp_dea_mean_channel_0.tif\n      processed\n    \n    \n      7\n      min_temp_channel0\n      perc95\n      DEA\n      min_temp_channel0_perc95\n      ../../dataresults/mvp_dea_perc95_channel_0.tif\n      processed\n    \n    \n      8\n      min_temp_channel0\n      perc5\n      DEA\n      min_temp_channel0_perc5\n      ../../dataresults/mvp_dea_perc5_channel_0.tif\n      processed\n    \n    \n      9\n      min_temp_channel1\n      mean\n      DEA\n      min_temp_channel1_mean\n      ../../dataresults/mvp_dea_mean_channel_1.tif\n      processed\n    \n    \n      10\n      min_temp_channel1\n      perc95\n      DEA\n      min_temp_channel1_perc95\n      ../../dataresults/mvp_dea_perc95_channel_1.tif\n      processed\n    \n    \n      11\n      min_temp_channel1\n      perc5\n      DEA\n      min_temp_channel1_perc5\n      ../../dataresults/mvp_dea_perc5_channel_1.tif\n      processed\n    \n    \n      12\n      min_temp_channel2\n      mean\n      DEA\n      min_temp_channel2_mean\n      ../../dataresults/mvp_dea_mean_channel_2.tif\n      processed\n    \n    \n      13\n      min_temp_channel2\n      perc95\n      DEA\n      min_temp_channel2_perc95\n      ../../dataresults/mvp_dea_perc95_channel_2.tif\n      processed\n    \n    \n      14\n      min_temp_channel2\n      perc5\n      DEA\n      min_temp_channel2_perc5\n      ../../dataresults/mvp_dea_perc5_channel_2.tif\n      processed\n    \n  \n\n\n\n\n\n\n\nDEM Download\nHere we download and extract the National Digital Elevation Model (DEM), and also generate slope and aspect rasters from the extracted DEM. For more details see getdata_dem.py\n\noutpath = os.path.join(settings.outpath, \"mvp_dem\")\ndem_layernames = settings.target_sources['DEM']\noutfnames = getdata_dem.get_dem_layers(dem_layernames, outpath, settings.target_bbox, settings.target_res)\n\n# Add extracted data to log dataframe\ndf_log = update_logtable(\n    df_log, outfnames, \n    dem_layernames, \n    'DEM', \n    settings, \n    layertitles = dem_layernames,\n    loginfos = 'downloaded')\ndf_log\n\nDEM downloaded to: ../../dataresults/mvp_dem/DEM_SRTM_1_Second_Hydro_Enforced_2022-07-04.tif\nDEM slope from: ../../dataresults/mvp_dem/DEM_SRTM_1_Second_Hydro_Enforced_2022-07-04.tif  saved to: ../../dataresults/mvp_dem/Slope_DEM_SRTM_1_Second_Hydro_Enforced_2022-07-04.tif\nDEM aspect from: ../../dataresults/mvp_dem/DEM_SRTM_1_Second_Hydro_Enforced_2022-07-04.tif  saved to: ../../dataresults/mvp_dem/Aspect_DEM_SRTM_1_Second_Hydro_Enforced_2022-07-04.tif\n\n\n\n\n\n\n  \n    \n      \n      layername\n      agfunction\n      dataset\n      layertitle\n      filename_out\n      loginfo\n    \n  \n  \n    \n      0\n      Bulk_Density\n      0-5cm\n      SLGA\n      Bulk_Density_0-5cm\n      ../../dataresults/SLGA_Bulk_Density_0-5cm.tif\n      downloaded\n    \n    \n      1\n      Clay\n      0-5cm\n      SLGA\n      Clay_0-5cm\n      ../../dataresults/SLGA_Clay_0-5cm.tif\n      downloaded\n    \n    \n      2\n      monthly_rain\n      Total\n      SILO\n      monthly_rain_Total\n      ../../dataresults/mvp_monthly_rain_silo/monthl...\n      downloaded\n    \n    \n      3\n      max_temp\n      Median\n      SILO\n      max_temp_Median\n      ../../dataresults/mvp_max_temp_silo/max_temp_2...\n      downloaded\n    \n    \n      4\n      min_temp\n      Median\n      SILO\n      min_temp_Median\n      ../../dataresults/mvp_min_temp_silo/min_temp_2...\n      downloaded\n    \n    \n      5\n      mean_temp\n      mean\n      SILO\n      mean_temp\n      ../../dataresults/silo_temp_2019_ag_mean.tif\n      processed\n    \n    \n      6\n      min_temp_channel0\n      mean\n      DEA\n      min_temp_channel0_mean\n      ../../dataresults/mvp_dea_mean_channel_0.tif\n      processed\n    \n    \n      7\n      min_temp_channel0\n      perc95\n      DEA\n      min_temp_channel0_perc95\n      ../../dataresults/mvp_dea_perc95_channel_0.tif\n      processed\n    \n    \n      8\n      min_temp_channel0\n      perc5\n      DEA\n      min_temp_channel0_perc5\n      ../../dataresults/mvp_dea_perc5_channel_0.tif\n      processed\n    \n    \n      9\n      min_temp_channel1\n      mean\n      DEA\n      min_temp_channel1_mean\n      ../../dataresults/mvp_dea_mean_channel_1.tif\n      processed\n    \n    \n      10\n      min_temp_channel1\n      perc95\n      DEA\n      min_temp_channel1_perc95\n      ../../dataresults/mvp_dea_perc95_channel_1.tif\n      processed\n    \n    \n      11\n      min_temp_channel1\n      perc5\n      DEA\n      min_temp_channel1_perc5\n      ../../dataresults/mvp_dea_perc5_channel_1.tif\n      processed\n    \n    \n      12\n      min_temp_channel2\n      mean\n      DEA\n      min_temp_channel2_mean\n      ../../dataresults/mvp_dea_mean_channel_2.tif\n      processed\n    \n    \n      13\n      min_temp_channel2\n      perc95\n      DEA\n      min_temp_channel2_perc95\n      ../../dataresults/mvp_dea_perc95_channel_2.tif\n      processed\n    \n    \n      14\n      min_temp_channel2\n      perc5\n      DEA\n      min_temp_channel2_perc5\n      ../../dataresults/mvp_dea_perc5_channel_2.tif\n      processed\n    \n    \n      15\n      DEM\n      None\n      DEM\n      DEM\n      ../../dataresults/mvp_dem/DEM_SRTM_1_Second_Hy...\n      downloaded\n    \n    \n      16\n      Slope\n      None\n      DEM\n      Slope\n      ../../dataresults/mvp_dem/Slope_DEM_SRTM_1_Sec...\n      downloaded\n    \n    \n      17\n      Aspect\n      None\n      DEM\n      Aspect\n      ../../dataresults/mvp_dem/Aspect_DEM_SRTM_1_Se...\n      downloaded\n    \n  \n\n\n\n\n\n\nLandscape\nDownload landscape data from Soil and Landscape Grid of Australia (SLGA).\n\n# Download landscape data\nlayernames = settings.target_sources['Landscape']\nlayertitles = ['Landscape_' + layername for layername in layernames]\n\noutfnames = getdata_landscape.get_landscape_layers(\n    layernames, \n    settings.target_bbox, \n    settings.outpath, \n    resolution = settings.target_res)\n\n# Add extracted data to log dataframe\ndf_log = update_logtable(\n    df_log, outfnames, \n    layernames, \n    'Landscape', \n    settings, \n    layertitles = layertitles,\n    loginfos = 'downloaded')\ndf_log\n\nDownloading Slope...\nSlope downloaded. Saved to:  ../../dataresults/Landscape_Slope.tif\nDownloading Aspect...\nAspect downloaded. Saved to:  ../../dataresults/Landscape_Aspect.tif\nDownloading Relief_300m...\nRelief_300m downloaded. Saved to:  ../../dataresults/Landscape_Relief_300m.tif\nLandscape Download complete.\n\n\n\n\n\n\n  \n    \n      \n      layername\n      agfunction\n      dataset\n      layertitle\n      filename_out\n      loginfo\n    \n  \n  \n    \n      0\n      Bulk_Density\n      0-5cm\n      SLGA\n      Bulk_Density_0-5cm\n      ../../dataresults/SLGA_Bulk_Density_0-5cm.tif\n      downloaded\n    \n    \n      1\n      Clay\n      0-5cm\n      SLGA\n      Clay_0-5cm\n      ../../dataresults/SLGA_Clay_0-5cm.tif\n      downloaded\n    \n    \n      2\n      monthly_rain\n      Total\n      SILO\n      monthly_rain_Total\n      ../../dataresults/mvp_monthly_rain_silo/monthl...\n      downloaded\n    \n    \n      3\n      max_temp\n      Median\n      SILO\n      max_temp_Median\n      ../../dataresults/mvp_max_temp_silo/max_temp_2...\n      downloaded\n    \n    \n      4\n      min_temp\n      Median\n      SILO\n      min_temp_Median\n      ../../dataresults/mvp_min_temp_silo/min_temp_2...\n      downloaded\n    \n    \n      5\n      mean_temp\n      mean\n      SILO\n      mean_temp\n      ../../dataresults/silo_temp_2019_ag_mean.tif\n      processed\n    \n    \n      6\n      min_temp_channel0\n      mean\n      DEA\n      min_temp_channel0_mean\n      ../../dataresults/mvp_dea_mean_channel_0.tif\n      processed\n    \n    \n      7\n      min_temp_channel0\n      perc95\n      DEA\n      min_temp_channel0_perc95\n      ../../dataresults/mvp_dea_perc95_channel_0.tif\n      processed\n    \n    \n      8\n      min_temp_channel0\n      perc5\n      DEA\n      min_temp_channel0_perc5\n      ../../dataresults/mvp_dea_perc5_channel_0.tif\n      processed\n    \n    \n      9\n      min_temp_channel1\n      mean\n      DEA\n      min_temp_channel1_mean\n      ../../dataresults/mvp_dea_mean_channel_1.tif\n      processed\n    \n    \n      10\n      min_temp_channel1\n      perc95\n      DEA\n      min_temp_channel1_perc95\n      ../../dataresults/mvp_dea_perc95_channel_1.tif\n      processed\n    \n    \n      11\n      min_temp_channel1\n      perc5\n      DEA\n      min_temp_channel1_perc5\n      ../../dataresults/mvp_dea_perc5_channel_1.tif\n      processed\n    \n    \n      12\n      min_temp_channel2\n      mean\n      DEA\n      min_temp_channel2_mean\n      ../../dataresults/mvp_dea_mean_channel_2.tif\n      processed\n    \n    \n      13\n      min_temp_channel2\n      perc95\n      DEA\n      min_temp_channel2_perc95\n      ../../dataresults/mvp_dea_perc95_channel_2.tif\n      processed\n    \n    \n      14\n      min_temp_channel2\n      perc5\n      DEA\n      min_temp_channel2_perc5\n      ../../dataresults/mvp_dea_perc5_channel_2.tif\n      processed\n    \n    \n      15\n      DEM\n      None\n      DEM\n      DEM\n      ../../dataresults/mvp_dem/DEM_SRTM_1_Second_Hy...\n      downloaded\n    \n    \n      16\n      Slope\n      None\n      DEM\n      Slope\n      ../../dataresults/mvp_dem/Slope_DEM_SRTM_1_Sec...\n      downloaded\n    \n    \n      17\n      Aspect\n      None\n      DEM\n      Aspect\n      ../../dataresults/mvp_dem/Aspect_DEM_SRTM_1_Se...\n      downloaded\n    \n    \n      18\n      Slope\n      None\n      Landscape\n      Landscape_Slope\n      ../../dataresults/Landscape_Slope.tif\n      downloaded\n    \n    \n      19\n      Aspect\n      None\n      Landscape\n      Landscape_Aspect\n      ../../dataresults/Landscape_Aspect.tif\n      downloaded\n    \n    \n      20\n      Relief_300m\n      None\n      Landscape\n      Landscape_Relief_300m\n      ../../dataresults/Landscape_Relief_300m.tif\n      downloaded\n    \n  \n\n\n\n\n\n\nRadiometrics\nDownload maps of Geoscience Australia National Geophysical Compilation Sub-collection Radiometrics\n\n# Download radiometrics\nlayernames = settings.target_sources['Radiometric']\n\noutfnames = getdata_radiometric.get_radiometric_layers(\n    settings.outpath, \n    layernames, \n    bbox = settings.target_bbox, \n    resolution=settings.target_res)\n\n # Add extracted data to log dataframe\ndf_log = update_logtable(\n    df_log, outfnames, \n    layernames, \n    'Radiometric', \n    settings, \n    layertitles = layernames,\n    loginfos = 'downloaded')\ndf_log\n\nDownloading image for layer radmap2019_grid_dose_terr_awags_rad_2019 ...\nLayer radmap2019_grid_dose_terr_awags_rad_2019 saved in ../../dataresults/radiometric_radmap2019_grid_dose_terr_awags_rad_2019.tif\nDownloading image for layer radmap2019_grid_dose_terr_filtered_awags_rad_2019 ...\nLayer radmap2019_grid_dose_terr_filtered_awags_rad_2019 saved in ../../dataresults/radiometric_radmap2019_grid_dose_terr_filtered_awags_rad_2019.tif\n\n\n\n\n\n\n  \n    \n      \n      layername\n      agfunction\n      dataset\n      layertitle\n      filename_out\n      loginfo\n    \n  \n  \n    \n      0\n      Bulk_Density\n      0-5cm\n      SLGA\n      Bulk_Density_0-5cm\n      ../../dataresults/SLGA_Bulk_Density_0-5cm.tif\n      downloaded\n    \n    \n      1\n      Clay\n      0-5cm\n      SLGA\n      Clay_0-5cm\n      ../../dataresults/SLGA_Clay_0-5cm.tif\n      downloaded\n    \n    \n      2\n      monthly_rain\n      Total\n      SILO\n      monthly_rain_Total\n      ../../dataresults/mvp_monthly_rain_silo/monthl...\n      downloaded\n    \n    \n      3\n      max_temp\n      Median\n      SILO\n      max_temp_Median\n      ../../dataresults/mvp_max_temp_silo/max_temp_2...\n      downloaded\n    \n    \n      4\n      min_temp\n      Median\n      SILO\n      min_temp_Median\n      ../../dataresults/mvp_min_temp_silo/min_temp_2...\n      downloaded\n    \n    \n      5\n      mean_temp\n      mean\n      SILO\n      mean_temp\n      ../../dataresults/silo_temp_2019_ag_mean.tif\n      processed\n    \n    \n      6\n      min_temp_channel0\n      mean\n      DEA\n      min_temp_channel0_mean\n      ../../dataresults/mvp_dea_mean_channel_0.tif\n      processed\n    \n    \n      7\n      min_temp_channel0\n      perc95\n      DEA\n      min_temp_channel0_perc95\n      ../../dataresults/mvp_dea_perc95_channel_0.tif\n      processed\n    \n    \n      8\n      min_temp_channel0\n      perc5\n      DEA\n      min_temp_channel0_perc5\n      ../../dataresults/mvp_dea_perc5_channel_0.tif\n      processed\n    \n    \n      9\n      min_temp_channel1\n      mean\n      DEA\n      min_temp_channel1_mean\n      ../../dataresults/mvp_dea_mean_channel_1.tif\n      processed\n    \n    \n      10\n      min_temp_channel1\n      perc95\n      DEA\n      min_temp_channel1_perc95\n      ../../dataresults/mvp_dea_perc95_channel_1.tif\n      processed\n    \n    \n      11\n      min_temp_channel1\n      perc5\n      DEA\n      min_temp_channel1_perc5\n      ../../dataresults/mvp_dea_perc5_channel_1.tif\n      processed\n    \n    \n      12\n      min_temp_channel2\n      mean\n      DEA\n      min_temp_channel2_mean\n      ../../dataresults/mvp_dea_mean_channel_2.tif\n      processed\n    \n    \n      13\n      min_temp_channel2\n      perc95\n      DEA\n      min_temp_channel2_perc95\n      ../../dataresults/mvp_dea_perc95_channel_2.tif\n      processed\n    \n    \n      14\n      min_temp_channel2\n      perc5\n      DEA\n      min_temp_channel2_perc5\n      ../../dataresults/mvp_dea_perc5_channel_2.tif\n      processed\n    \n    \n      15\n      DEM\n      None\n      DEM\n      DEM\n      ../../dataresults/mvp_dem/DEM_SRTM_1_Second_Hy...\n      downloaded\n    \n    \n      16\n      Slope\n      None\n      DEM\n      Slope\n      ../../dataresults/mvp_dem/Slope_DEM_SRTM_1_Sec...\n      downloaded\n    \n    \n      17\n      Aspect\n      None\n      DEM\n      Aspect\n      ../../dataresults/mvp_dem/Aspect_DEM_SRTM_1_Se...\n      downloaded\n    \n    \n      18\n      Slope\n      None\n      Landscape\n      Landscape_Slope\n      ../../dataresults/Landscape_Slope.tif\n      downloaded\n    \n    \n      19\n      Aspect\n      None\n      Landscape\n      Landscape_Aspect\n      ../../dataresults/Landscape_Aspect.tif\n      downloaded\n    \n    \n      20\n      Relief_300m\n      None\n      Landscape\n      Landscape_Relief_300m\n      ../../dataresults/Landscape_Relief_300m.tif\n      downloaded\n    \n    \n      21\n      radmap2019_grid_dose_terr_awags_rad_2019\n      None\n      Radiometric\n      radmap2019_grid_dose_terr_awags_rad_2019\n      ../../dataresults/radiometric_radmap2019_grid_...\n      downloaded\n    \n    \n      22\n      radmap2019_grid_dose_terr_filtered_awags_rad_2019\n      None\n      Radiometric\n      radmap2019_grid_dose_terr_filtered_awags_rad_2019\n      ../../dataresults/radiometric_radmap2019_grid_...\n      downloaded"
  },
  {
    "objectID": "pydocs/Data_Harvest.html#save-the-final-log-or-start-from-here-to-re-load-it-in.",
    "href": "pydocs/Data_Harvest.html#save-the-final-log-or-start-from-here-to-re-load-it-in.",
    "title": "AgReFed Data-Harvester",
    "section": "Save the final log or start from here to re-load it in.",
    "text": "Save the final log or start from here to re-load it in.\nWe have now completed the data download section. You may add additional downlods and processing steps to your log file.\n\n# Save out (or load in) the log file.\nlogfile = settings.outpath+'log.csv'\nif exists(logfile):\n    df_log = pd.read_csv(settings.outpath+'log.csv')\nelse:\n    df_log.to_csv(settings.outpath+'log.csv',index=False)\n\ndf_log\n\n\n\n\n\n  \n    \n      \n      layername\n      agfunction\n      dataset\n      layertitle\n      filename_out\n      loginfo\n    \n  \n  \n    \n      0\n      Bulk_Density\n      0-5cm\n      SLGA\n      Bulk_Density_0-5cm\n      ../../dataresults/SLGA_Bulk_Density_0-5cm.tif\n      downloaded\n    \n    \n      1\n      Clay\n      0-5cm\n      SLGA\n      Clay_0-5cm\n      ../../dataresults/SLGA_Clay_0-5cm.tif\n      downloaded\n    \n    \n      2\n      monthly_rain\n      Total\n      SILO\n      monthly_rain_Total\n      ../../dataresults/mvp_monthly_rain_silo/monthl...\n      downloaded\n    \n    \n      3\n      max_temp\n      Median\n      SILO\n      max_temp_Median\n      ../../dataresults/mvp_max_temp_silo/max_temp_2...\n      downloaded\n    \n    \n      4\n      min_temp\n      Median\n      SILO\n      min_temp_Median\n      ../../dataresults/mvp_min_temp_silo/min_temp_2...\n      downloaded\n    \n    \n      5\n      mean_temp\n      mean\n      SILO\n      mean_temp\n      ../../dataresults/silo_temp_2019_ag_mean.tif\n      processed\n    \n    \n      6\n      min_temp_channel0\n      mean\n      DEA\n      min_temp_channel0_mean\n      ../../dataresults/mvp_dea_mean_channel_0.tif\n      processed\n    \n    \n      7\n      min_temp_channel0\n      perc95\n      DEA\n      min_temp_channel0_perc95\n      ../../dataresults/mvp_dea_perc95_channel_0.tif\n      processed\n    \n    \n      8\n      min_temp_channel0\n      perc5\n      DEA\n      min_temp_channel0_perc5\n      ../../dataresults/mvp_dea_perc5_channel_0.tif\n      processed\n    \n    \n      9\n      min_temp_channel1\n      mean\n      DEA\n      min_temp_channel1_mean\n      ../../dataresults/mvp_dea_mean_channel_1.tif\n      processed\n    \n    \n      10\n      min_temp_channel1\n      perc95\n      DEA\n      min_temp_channel1_perc95\n      ../../dataresults/mvp_dea_perc95_channel_1.tif\n      processed\n    \n    \n      11\n      min_temp_channel1\n      perc5\n      DEA\n      min_temp_channel1_perc5\n      ../../dataresults/mvp_dea_perc5_channel_1.tif\n      processed\n    \n    \n      12\n      min_temp_channel2\n      mean\n      DEA\n      min_temp_channel2_mean\n      ../../dataresults/mvp_dea_mean_channel_2.tif\n      processed\n    \n    \n      13\n      min_temp_channel2\n      perc95\n      DEA\n      min_temp_channel2_perc95\n      ../../dataresults/mvp_dea_perc95_channel_2.tif\n      processed\n    \n    \n      14\n      min_temp_channel2\n      perc5\n      DEA\n      min_temp_channel2_perc5\n      ../../dataresults/mvp_dea_perc5_channel_2.tif\n      processed\n    \n    \n      15\n      DEM\n      None\n      DEM\n      DEM\n      ../../dataresults/mvp_dem/DEM_SRTM_1_Second_Hy...\n      downloaded\n    \n    \n      16\n      Slope\n      None\n      DEM\n      Slope\n      ../../dataresults/mvp_dem/Slope_DEM_SRTM_1_Sec...\n      downloaded\n    \n    \n      17\n      Aspect\n      None\n      DEM\n      Aspect\n      ../../dataresults/mvp_dem/Aspect_DEM_SRTM_1_Se...\n      downloaded\n    \n    \n      18\n      Slope\n      None\n      Landscape\n      Landscape_Slope\n      ../../dataresults/Landscape_Slope.tif\n      downloaded\n    \n    \n      19\n      Aspect\n      None\n      Landscape\n      Landscape_Aspect\n      ../../dataresults/Landscape_Aspect.tif\n      downloaded\n    \n    \n      20\n      Relief_300m\n      None\n      Landscape\n      Landscape_Relief_300m\n      ../../dataresults/Landscape_Relief_300m.tif\n      downloaded\n    \n    \n      21\n      radmap2019_grid_dose_terr_awags_rad_2019\n      None\n      Radiometric\n      radmap2019_grid_dose_terr_awags_rad_2019\n      ../../dataresults/radiometric_radmap2019_grid_...\n      downloaded\n    \n    \n      22\n      radmap2019_grid_dose_terr_filtered_awags_rad_2019\n      None\n      Radiometric\n      radmap2019_grid_dose_terr_filtered_awags_rad_2019\n      ../../dataresults/radiometric_radmap2019_grid_...\n      downloaded"
  },
  {
    "objectID": "pydocs/Data_Harvest.html#points-extraction-from-downloadedprocessed-data",
    "href": "pydocs/Data_Harvest.html#points-extraction-from-downloadedprocessed-data",
    "title": "AgReFed Data-Harvester",
    "section": "Points extraction from downloaded/processed data",
    "text": "Points extraction from downloaded/processed data\nBy default point values of all processed layers in df_log are extracted given by the input locations. However, you can select also only certain layers (see in code).\n\n# Select all processed data\ndf_sel = df_log.copy()\n\n# or select only the rasters of interest, for example:\n\"\"\"\ndf_sel = df_log[df_log['layername'].isin(['DEM','Slope',\n'landsat8_nbart_16day_channel0', \n'Organic_Carbon','Depth_of_Soil',\n'mean_temp','monthly_rain'])]\n\"\"\"\n\nrasters= df_sel['filename_out'].values.tolist()\ntitles = df_sel['layertitle'].values.tolist()\n    \n# Extract datatable from rasters given input coordinates\ngdf = utils.raster_query(longs,lats,rasters,titles)\n\nOpening: ../../dataresults/SLGA_Bulk_Density_0-5cm.tif\nRaster pixel size: (156, 216)\nOpening: ../../dataresults/SLGA_Clay_0-5cm.tif\nRaster pixel size: (156, 216)\nOpening: ../../dataresults/mvp_monthly_rain_silo/monthly_rain_2021_cropped.tif\nRaster pixel size: (2, 3)\nOpening: ../../dataresults/mvp_max_temp_silo/max_temp_2021_cropped.tif\nRaster pixel size: (2, 3)\nOpening: ../../dataresults/mvp_min_temp_silo/min_temp_2021_cropped.tif\nRaster pixel size: (2, 3)\nOpening: ../../dataresults/silo_temp_2019_ag_mean.tif\nRaster pixel size: (2, 3)\nOpening: ../../dataresults/mvp_dea_mean_channel_0.tif\nRaster pixel size: (77, 107)\nOpening: ../../dataresults/mvp_dea_perc95_channel_0.tif\nRaster pixel size: (77, 107)\nOpening: ../../dataresults/mvp_dea_perc5_channel_0.tif\nRaster pixel size: (77, 107)\nOpening: ../../dataresults/mvp_dea_mean_channel_1.tif\nRaster pixel size: (77, 107)\nOpening: ../../dataresults/mvp_dea_perc95_channel_1.tif\nRaster pixel size: (77, 107)\nOpening: ../../dataresults/mvp_dea_perc5_channel_1.tif\nRaster pixel size: (77, 107)\nOpening: ../../dataresults/mvp_dea_mean_channel_2.tif\nRaster pixel size: (77, 107)\nOpening: ../../dataresults/mvp_dea_perc95_channel_2.tif\nRaster pixel size: (77, 107)\nOpening: ../../dataresults/mvp_dea_perc5_channel_2.tif\nRaster pixel size: (77, 107)\nOpening: ../../dataresults/mvp_dem/DEM_SRTM_1_Second_Hydro_Enforced_2022-07-04.tif\nRaster pixel size: (78, 108)\nOpening: ../../dataresults/mvp_dem/Slope_DEM_SRTM_1_Second_Hydro_Enforced_2022-07-04.tif\nRaster pixel size: (78, 108)\nOpening: ../../dataresults/mvp_dem/Aspect_DEM_SRTM_1_Second_Hydro_Enforced_2022-07-04.tif\nRaster pixel size: (78, 108)\nOpening: ../../dataresults/Landscape_Slope.tif\nRaster pixel size: (78, 108)\nOpening: ../../dataresults/Landscape_Aspect.tif\nRaster pixel size: (78, 108)\nOpening: ../../dataresults/Landscape_Relief_300m.tif\nRaster pixel size: (78, 108)\nOpening: ../../dataresults/radiometric_radmap2019_grid_dose_terr_awags_rad_2019.tif\nRaster pixel size: (466, 647)\nOpening: ../../dataresults/radiometric_radmap2019_grid_dose_terr_filtered_awags_rad_2019.tif\nRaster pixel size: (466, 647)\n\n\n\nInspect result dataframe\n\n# Inspect either entire generated dataframe with \n# gdf\n# or only the first rows\ngdf.head()\n\n\n\n\n\n  \n    \n      \n      Longitude\n      Latitude\n      geometry\n      Bulk_Density_0-5cm\n      Clay_0-5cm\n      monthly_rain_Total\n      max_temp_Median\n      min_temp_Median\n      mean_temp\n      min_temp_channel0_mean\n      ...\n      min_temp_channel2_perc95\n      min_temp_channel2_perc5\n      DEM\n      Slope\n      Aspect\n      Landscape_Slope\n      Landscape_Aspect\n      Landscape_Relief_300m\n      radmap2019_grid_dose_terr_awags_rad_2019\n      radmap2019_grid_dose_terr_filtered_awags_rad_2019\n    \n  \n  \n    \n      0\n      149.852680\n      -30.264663\n      POINT (149.85268 -30.26466)\n      1.368779\n      27.214527\n      47.000000\n      37.500000\n      24.700001\n      20.678619\n      1059.0\n      ...\n      541.0\n      541.0\n      244.658585\n      89.899475\n      265.249023\n      1.046624\n      209.138062\n      10.463379\n      33.151680\n      32.962944\n    \n    \n      5\n      149.884838\n      -30.265302\n      POINT (149.88484 -30.26530)\n      1.362662\n      31.956041\n      47.399902\n      37.299999\n      24.500000\n      20.548504\n      1082.0\n      ...\n      540.0\n      540.0\n      264.428772\n      89.937111\n      291.358032\n      1.001000\n      279.542847\n      6.037811\n      35.969486\n      35.945480\n    \n    \n      9\n      149.884838\n      -30.265302\n      POINT (149.88484 -30.26530)\n      1.362662\n      31.956041\n      47.399902\n      37.299999\n      24.500000\n      20.548504\n      1082.0\n      ...\n      540.0\n      540.0\n      264.428772\n      89.937111\n      291.358032\n      1.001000\n      279.542847\n      6.037811\n      35.969486\n      35.945480\n    \n    \n      14\n      149.838791\n      -30.278542\n      POINT (149.83879 -30.27854)\n      1.360451\n      32.675858\n      35.899902\n      37.600002\n      24.900000\n      20.803707\n      1092.0\n      ...\n      601.0\n      601.0\n      233.005081\n      89.918648\n      250.619858\n      0.841430\n      242.743683\n      4.798782\n      29.618393\n      29.478428\n    \n    \n      19\n      149.830843\n      -30.275437\n      POINT (149.83084 -30.27544)\n      1.334362\n      35.097813\n      35.899902\n      37.600002\n      24.900000\n      20.803707\n      1160.0\n      ...\n      626.0\n      626.0\n      230.575439\n      89.921860\n      194.598907\n      1.062537\n      242.921112\n      5.204880\n      25.061012\n      24.757614\n    \n  \n\n5 rows × 26 columns\n\n\n\n\n# Get some general info about result table:\ngdf.info()\n\n<class 'geopandas.geodataframe.GeoDataFrame'>\nInt64Index: 82 entries, 0 to 309\nData columns (total 26 columns):\n #   Column                                             Non-Null Count  Dtype   \n---  ------                                             --------------  -----   \n 0   Longitude                                          82 non-null     float64 \n 1   Latitude                                           82 non-null     float64 \n 2   geometry                                           82 non-null     geometry\n 3   Bulk_Density_0-5cm                                 82 non-null     float32 \n 4   Clay_0-5cm                                         82 non-null     float32 \n 5   monthly_rain_Total                                 82 non-null     float32 \n 6   max_temp_Median                                    82 non-null     float32 \n 7   min_temp_Median                                    82 non-null     float32 \n 8   mean_temp                                          82 non-null     float32 \n 9   min_temp_channel0_mean                             82 non-null     float32 \n 10  min_temp_channel0_perc95                           82 non-null     float32 \n 11  min_temp_channel0_perc5                            82 non-null     float32 \n 12  min_temp_channel1_mean                             82 non-null     float32 \n 13  min_temp_channel1_perc95                           82 non-null     float32 \n 14  min_temp_channel1_perc5                            82 non-null     float32 \n 15  min_temp_channel2_mean                             82 non-null     float32 \n 16  min_temp_channel2_perc95                           82 non-null     float32 \n 17  min_temp_channel2_perc5                            82 non-null     float32 \n 18  DEM                                                82 non-null     float32 \n 19  Slope                                              82 non-null     float32 \n 20  Aspect                                             82 non-null     float32 \n 21  Landscape_Slope                                    82 non-null     float32 \n 22  Landscape_Aspect                                   82 non-null     float32 \n 23  Landscape_Relief_300m                              82 non-null     float32 \n 24  radmap2019_grid_dose_terr_awags_rad_2019           82 non-null     float32 \n 25  radmap2019_grid_dose_terr_filtered_awags_rad_2019  82 non-null     float32 \ndtypes: float32(23), float64(2), geometry(1)\nmemory usage: 9.9 KB\n\n\n\n\nSave the results table\nFinally, the result dataframe table is saved as a csv file, which can be used now to do some awesome ML. In addition the results are also saved as a geo-spatial referenced geopackage (.gpkg), which can be used again as input for further analysis or to inspect and overlay data on other layers and basemaps. The geopackage is a standard georeferenced file format and can be opened with any geo-spatial package or interactive software (e.g., QGIS, Esri ArcGIS).\n\n# Save the results table to a csv \ngdf.to_csv(os.path.join(settings.outpath, \"results.csv\"), index = True, mode='w')\n\n# Save also as geopackage\ngdf.to_file(os.path.join(settings.outpath, \"results.gpkg\"), driver=\"GPKG\")\n# Note: The deprecated warning below is a bug in geopandas and will be fixed in their bext version.\n\n/Users/seb/mambaforge/envs/py39_agrefed/lib/python3.9/site-packages/geopandas/io/file.py:362: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n  pd.Int64Index,\n\n\n\n\nOverview plot of all processed rasters\nThis provides a quick overview to inspect all processed data layers with an overlay of the requested location points.\n\n# Plot one of that datasets with the points on top\nutils.plot_rasters(rasters,longs,lats,titles)\n\n\n\n\n\n# print total time (only needed for testing if notebook kernel runs all at once):\nprint('FINISHED')\nend_time = datetime.now()\nprint('Duration: {}'.format(end_time - start_time))\n\nFINISHED\nDuration: 0:34:37.200976"
  },
  {
    "objectID": "pydocs/py00-workshop.html",
    "href": "pydocs/py00-workshop.html",
    "title": "Python Workshop",
    "section": "",
    "text": "Python workshop\n\n\n\nInterested in a Data-Harvester workshop run in  Python? Let us know. We’ll be sending out an email to everyone who signed up for the course when it’s ready to go."
  },
  {
    "objectID": "code-of-conduct.html",
    "href": "code-of-conduct.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "We expect all attendees of our training to follow our code of conduct, including bullying, harassment and discrimination prevention policies.\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOnce you have fully read and understood the Code of Conduct, you may proceed by clicking on the relevant workshop link.\n\nR Workshop\nPython Workshop"
  },
  {
    "objectID": "rdocs/r03-basic.html",
    "href": "rdocs/r03-basic.html",
    "title": " Custom workflows",
    "section": "",
    "text": "In this workshop\n\n\n\n\n\nManually download data without the YAML configuration file\nAccess and view Google Earth Engine data\nExtract samples from downloaded rasters"
  },
  {
    "objectID": "rdocs/r03-basic.html#introduction",
    "href": "rdocs/r03-basic.html#introduction",
    "title": " Custom workflows",
    "section": "Introduction",
    "text": "Introduction\nWelcome to Workshop 2.\nThe harvest() function used in the last workshop was a wrapper for several download_*() functions already available in dataharvester (see source). In this workshop, we willl show you how to call these functions directly for more control over the data download process:\n\ndownload_dem() for Geoscience Australia DEM grid data\ndownload_slga() for Soil and Landscape Grid Australia (SLGA) - Soil Atributes\ndownload_landscape() for SLGA - Landscape Attributes\ndownload_radiometric() for Geosciences Australia’s Radiometric Map of Australia\ndownload_silo() for Scientific Information for Land Owners (SILO) climate data\ncollect_ee(), preprocess_ee() and download_ee() for access to datasets available in the Google Earth Engine Data Catalog\n\nWe will also show you how to extract single-band samples from downloaded rasters. This is useful if you already have raster images and want to extract samples from them.\n\nBefore you begin\nIf you have not already done so, initialise the Data-Harvester:\nlibrary(dataharvester)\ninitialise_harvester(\"r-reticulate\")"
  },
  {
    "objectID": "rdocs/r03-basic.html#fa-person-chalkboard-demo-1-manual-downloads",
    "href": "rdocs/r03-basic.html#fa-person-chalkboard-demo-1-manual-downloads",
    "title": " Custom workflows",
    "section": " Demo 1: Manual downloads",
    "text": "Demo 1: Manual downloads\n\nFunction syntax\nEach download_*() function has similar syntax:\ndownload_*(layer,\n           out_path, \n           bounding_box, \n           <<other arguments>>)\nwhere:\n\nlayer determines the name(s) of the layer(s) to download\nout_path is the path to the folder where the data should be saved\nbounding_box is the EPSG:4328 coordinates used to define the area to download as a bounding box\n\nFor example, to download the Hydrologically Enforced DEM (DEM-H) layer from Geoscience Australia’s DEM grid data, you would use:\ndownload_dem(\n    layer = \"DEM\",\n    out_path = \"downloads/\",\n    bounding_box = c(149.769345, -30.335861, 149.949173, -30.206271)\n)\n\n\n\n\n\n\nReminder\n\n\n\nThe bounding box is defined as c(xmin, ymin, xmax, ymax) in EPSG:4326\n\n\nLet’s define some areas of interest as bounding boxes for the rest of this Demo:\nllara <- c(149.769, -30.335, 149.969, -30.135)       # NSW\ncorrigin <- c(118.015, -32.356, 118.215, -32.156)    # WA\nnedscorner <- c(141.215, -34.241, 141.415, -34.0414) # VIC\nThese are locations on the Australian continent as most APIs we use are specific to Australia (with the exception of functions that use the Google Earth Engine API).\n\n\n\n\n\n\nInterested in generating your own bounding box?\n\n\n\n\n\nYou can try to find the bounding box for your area of interest using this bounding box tool. Make sure to select “CSV” output for easier copy and paste.\n\n\n\n\n\nHow do we know what layers are available?\nEach download_*() function has a layers argument that accepts a pre-defined list of layers. These are documented in the function help() or ?. For example, to see the available layers for the download_dem() function, you can use:\n?download_dem\nThe documentation will show you the available layers and their descriptions. Try the following:\n?download_slga\n?download_landscape\n?download_radiometric\n\nThere are also some additional arguments available to each download_*() function. For example, download_slga() has the arguments depth_min, depth_max and get_ci to specify the minimum and maximum soil depth and whether to download the soil confidence interval (CI) layers:\n\ndownload_slga(\n    layer = \"Clay\",\n    out_path = \"downloads/\",\n    bounding_box = llara,\n    depth_min = 0,\n    depth_max = 5,\n    get_ci = TRUE\n)\nIt is quite clear that you will want to read the documentation for each function to understand what arguments are available and how to use them."
  },
  {
    "objectID": "rdocs/r03-basic.html#fa-person-chalkboard-demo-2-google-earth-engine",
    "href": "rdocs/r03-basic.html#fa-person-chalkboard-demo-2-google-earth-engine",
    "title": " Custom workflows",
    "section": " Demo 2: Google Earth Engine",
    "text": "Demo 2: Google Earth Engine\nThe Google Earth Engine (GEE) API is a cloud-based platform for planetary-scale geospatial analysis. It provides access to a large collection of satellite imagery, elevation data, and other geospatial datasets. We provide limited access to the GEE API through the dataharvester package, which allows you to download data from the GEE Data Catalogue.\nFor full functionality, we recommend that you use the GEE Python API directly, or use the R wrapper package, rgee. If you are not familiar with the GEE API or wish to explore what is possible, then dataharvester provides a simplified interface to the GEE API while giving users access to a full range of features.\n\nFunction syntax\nAccessing GEE is simple once you have an account and have initialised the dataharvester package:\n\nThe collect_ee() function is used to search the GEE Data Catalogue and return a list of available datasets based on pre-determined filters.\nThe preprocess_ee() function is used to calculate summaries within the datasets before downloading or mapping.\nThe map_ee() and download_ee() functions are then available to map and download the data.\n\n\n\nWorkflow\nThe workflow for using the GEE API is as follows:\n\nUse collect_ee() to search the GEE Data Catalogue and return a list of available datasets based on pre-determined filters.\nUse preprocess_ee() to calculate summaries. For example, you can composite multiple images into a single image. Or, you can calculate a spectral index like NDVI. If accessing some supported satellites, you may even be able to mask clouds and shadows, or apply scaling and offsets automatically.\nUse map_ee() to map the data (optional), and download_ee() to download the data.\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that the order of processing must follow the above, or there may be unintended consequences. If you find any irregular behaviour, let us know! The dataharvester package is in early development and we are keen to fix any undocumented issues.\n\n\nOnce the data has been downloaded as images, you may manually extract samples like how you would with any other GeoTIFF - we will discuss that in Workshop 3.\nFor now, let’s use a full example to demonstrate and explain these functions. Make sure that you have initialised the dataharvester package with your GEE credentials:\ninitialise_harvester(\"r-reticulate\", earthengine = TRUE)\nimg <- collect_ee(\n  collection = \"LANDSAT/LC09/C02/T1_L2\",\n  coords = llara,\n  date = \"2021-06-01\",\n  end_date = \"2022-06-01\"\n)\nimg <- preprocess_ee(object = img, reduce = \"median\")\nimg <- map_ee(img, bands = c(\"SR_B2\", \"SR_B3\", \"SR_B4\"))\nimg <- download_ee(img, bands = c(\"SR_B2\", \"SR_B3\", \"SR_B4\"), \n  out_path = \"downloads/\"\nThe code above can also be streamlined with pipes:\nimg <- \n  collect_ee(\n    collection = \"LANDSAT/LC09/C02/T1_L2\",\n    coords = llara,\n    date = \"2021-06-01\",\n    end_date = \"2022-06-01\") %>% \n  preprocess_ee(reduce = \"median\") %>% \n  map_ee(bands = c(\"SR_B2\", \"SR_B3\", \"SR_B4\")) %>% \n  download_ee(bands = c(\"SR_B2\", \"SR_B3\", \"SR_B4\"), \n    out_path = \"downloads/\"\nYour instructor will demonstrate more examples during the workshop."
  },
  {
    "objectID": "rdocs/r03-basic.html#fa-keyboard-exercise-1-cloud-and-shadow-masking",
    "href": "rdocs/r03-basic.html#fa-keyboard-exercise-1-cloud-and-shadow-masking",
    "title": " Custom workflows",
    "section": " Exercise 1: Cloud and shadow masking",
    "text": "Exercise 1: Cloud and shadow masking\n\n\n\n\n\n\n On your own\n\n\n\nIn this exercise, you will use the dataharvester package to download Landsat 8 data from the GEE Data Catalogue. You will then use the preprocess_ee() function to mask clouds and shadows, and then download the data.\n\nUse collect_ee() to search the GEE Data Catalogue and return a list of available datasets based on pre-determined filters. Use the following arguments:\n\ncollection = \"LANDSAT/LC08/C01/T1_SR\"\ncoords = llara\ndate = \"2021-06-01\"\nend_date = \"2021-08-01\"\n\nUse preprocess_ee() to create two different products, i.e. save them as different objects, e.g. img_masked and img. Use the following arguments:\n\nThe first will mask clouds using the following argument:\n\nmask_clouds = TRUE\n\nThe second will not mask clouds:\n\nmask_clouds = FALSE\n\n\nUse map_ee() to map the data. Use the following arguments:\n\nbands = c(\"B4\", \"B3\", \"B2\")\n\n\nYour instructor will discuss the results."
  },
  {
    "objectID": "rdocs/r03-basic.html#fa-person-chalkboard-demo-3-preview-rasters",
    "href": "rdocs/r03-basic.html#fa-person-chalkboard-demo-3-preview-rasters",
    "title": " Custom workflows",
    "section": " Demo 3: Preview rasters",
    "text": "Demo 3: Preview rasters\n\n\n\n\n\n\nAgenda\n\n\n\nTime to preview the downloaded data. This demo will:\n\nIntroduce plotting of raster data using terra\nDiscuss how to plot single-band and multi-band rasters\n\n\n\nOnce data has been downloaded, it is a trivial task to preview the data using the terra package. The terra package is a powerful package for working with raster data in R. First, we will load the terra package which is automatically installed when you install the dataharvester package:\nlibrary(terra)\nAny image that you have downloaded using the dataharvester package will need to be converted to a SpatRaster object in R using the rast() function. For example, to conver the SLGA image that we downloaded earlier, we can use:\nraster <- rast(\"downloads/SLGA_Clay_0-5cm.tif\")\nThen, to plot, we can use the plot() function:\nplot(raster)\nThe plot() function will automatically plot the raster using the default colour palette. However, you can also specify the colour palette using the col argument. For example, to plot the raster using the viridis colour palette, we can use:\nplot(raster, col = \"viridis\")\n\nPlotting objects created by download_*() functions\nIf you use the download_*() functions and save the output to an object, then you can use the plot() function to plot the data immediately. The plots will be identical to what was produced in terra, since the package is used to create the S3 plot object.\nAs an example, to plot the data downloaded using the download_dea() function, we can use:\ndea <- download_dea(\n  layer = c(\"landsat_barest_earth\", \"ga_ls_fc_pc_cyear_3\"),\n  bounding_box = llara,\n  out_path = \"downloads/\",\n  years = 2021,\n  resolution = 6\n)\nplot(dea)\nThe code above will plot all layers and their bands in the same plot."
  },
  {
    "objectID": "rdocs/r03-basic.html#fa-keyboard-exercise-2-more-plots",
    "href": "rdocs/r03-basic.html#fa-keyboard-exercise-2-more-plots",
    "title": " Custom workflows",
    "section": " Exercise 2: More plots",
    "text": "Exercise 2: More plots\n\n\n\n\n\n\n On your own\n\n\n\nFor objects that are created using the download_*() functions, additional arguments can be used to refine the plots. For example, if you want to plot each layer separately, then you can use the plot() function with the choose argument:\nplot(x, choose = 1)\nIn a data layer, mutltiple bands can also be plotted separately. To do so, you can use the plot() function with the index argument:\nplot(x, choose = 1, index = 1)\n# or\n# plot(x, 1, 1)\n\nTask\nUsing the same code as above to download data from DEA:\nModify the code and:\n\nsave the output to an object called dea\nplot the data using the plot() function\nplot only the second layer \"ga_ls_fc_pc_cyear_3\"\nplot only the pv_pc_50 band from the second layer\n\n\n\n\n\n\n\n\n\n\n Solution\n\n\n\n\n\n# 1 \ndea <- download_dea(\n  layer = c(\"landsat_barest_earth\", \"ga_ls_fc_pc_cyear_3\"),\n  bounding_box = llara,\n  out_path = \"downloads/\",\n  years = 2021,\n  resolution = 6\n)\n\n# 2\nplot(dea)\n\n# 3\nplot(dea, 2)\n\n# 4 \nplot(dea, 2, 2)"
  },
  {
    "objectID": "rdocs/r03-basic.html#task",
    "href": "rdocs/r03-basic.html#task",
    "title": " Custom workflows",
    "section": "Task",
    "text": "Task\nUsing the same code as above to download data from DEA:\nModify the code and:\n\nsave the output to an object called dea\nplot the data using the plot() function\nplot only the second layer \"ga_ls_fc_pc_cyear_3\"\nplot only the pv_pc_50 band from the second layer"
  },
  {
    "objectID": "rdocs/r03-basic.html#wrapping-up",
    "href": "rdocs/r03-basic.html#wrapping-up",
    "title": " Custom workflows",
    "section": "Wrapping up",
    "text": "Wrapping up\nThis is the end of Workshop 2. In this workshop we looked at using manual methods to download data from various API sources and the GEE Catalog. We also briefly looked at how to plot the data using the terra package as well as the plot() function for objects created using the download_*() functions."
  },
  {
    "objectID": "rdocs/r01-setup.html",
    "href": "rdocs/r01-setup.html",
    "title": "Setting up R",
    "section": "",
    "text": "We encourage you to use RStudio Cloud for the workshop as you do not need to install anything on your computer. Simply log in with your free account and follow along with the workshop. Join our shared space by clicking the button below:\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the near future, we will be able to provide you with a virtual machine on the Nectar Research Cloud. This will allow you to use RStudio Server on a virtual machine. Instructions will be provided when this is available."
  },
  {
    "objectID": "rdocs/r01-setup.html#binder",
    "href": "rdocs/r01-setup.html#binder",
    "title": "Setting up R",
    "section": "Binder",
    "text": "Binder\nWant to play around with dataharvester without an RStudio Cloud account? Launch our Binder instance by clicking on the button below. Binder should only be used in a test environment as Binder instances are not persistent and will be deleted after a period of inactivity."
  },
  {
    "objectID": "rdocs/r01-setup.html#rstudio-desktop",
    "href": "rdocs/r01-setup.html#rstudio-desktop",
    "title": "Setting up R",
    "section": "RStudio Desktop",
    "text": "RStudio Desktop\n\n\n\n\n\n\nImportant\n\n\n\nChoosing this option means that you have decided to install R and RStudio on your computer. This is not recommended for beginners as it can be a complicated process. If you are unsure, please use RStudio Cloud.\nYou must know how to debug issues if you choose to use RStudio Desktop. Please set up RStudio before the workshop so that you can debug early and focus on the content (see below).\n\n\n\n\n\n\n\n\nStep 1: Install/Update R and RStudio\n\n\n\n\n\nIf you already have R and RStudio installed, make sure you have the latest versions. At a minimum, you will need R v4.21 and RStudio v2022.07.2.\n\n\n\n\n\n\n\n\n\nStep 2: Install and load the dataharvester package\n\n\n\n\n\nInstall the \"dataharvester\" package from GitHub. You will need either the \"devtools\" or \"remotes\" package (recommended) installed to do this.\n\n\n\n\n\n\nWarning\n\n\n\nIf you are a Windows user and choose to use \"devtools\", you may need to install Rtools first.\n\n\ninstall.packages(\"remotes\")\nremotes::install_github(\"Sydney-Informatics-Hub/dataharvester\")\nDon’t forget to load the package:\nlibrary(dataharvester)\n\n\n\n\n\n\n\n\n\nStep 3: Install dependencies\n\n\n\n\n\nThe dataharvester package has a number of dependencies that you will need to install, since the code is in Python. Luckily, there is a function to install these dependencies for you. Run the following code, which will install dependencies in a conda environment called “r-reticulate”:\ninitialise_harvester(\"r-reticulate\")\nThis function must always be run prior to using dataharvester functions. It may take a few minutes to run the first time. For subsequent sessions, initialise_harvester() should take only a few seconds to run.\n\n\n\n\n\n\nImportant\n\n\n\nDuring this step, you may be asked to install r-miniconda, which is basically miniconda. This is a package that allows you to install and use Python packages within R through the Conda binary package manager.\nIf you already have Python installed, R may still ask you to install miniconda. This is because miniconda is a separate package from Python, and is not the same as the Python package manager pip. Importantly, miniconda is almost necessary for Windows users as it allows you to install certain Python and non-Python packages without having to install a full version of Python and running installation commands in the command line. These include the GDAL binary which is a translator library for raster and vector geospatial data formats, and the PROJ library which is a cartographic projection and coordinate transformation library.\n\n\n\n\n\n\n\n\n\n\n\nStep 4 (Final): Authenticate with Google Earth Engine\n\n\n\n\n\nIf you have a Google Earth Engine account, you can initialise the package with your credentials. This will allow you to access Google Earth Engine data.\nTo activate Google Earth Engine, simply include earthengine = TRUE when initialising:\ninitialise_harvester(earthengine = TRUE)\nFollow the instructions carefully, as Google Earth Engine requires you to authenticate with a Google account. You will need to copy and paste a link into your browser, and then copy and paste the resulting code back into R.\n\n\n\n\nNeed help?\nIf you are stuck at any point, we have a dedicated Troubleshooting section that you can refer to. This section will be updated as we receive more questions."
  },
  {
    "objectID": "rdocs/r00-workshop.html",
    "href": "rdocs/r00-workshop.html",
    "title": "R Workshop",
    "section": "",
    "text": "The AgReFed Data-Harvester can be run in Python or R.\nThis R workshop is aimed at people who are already familiar with R and RStudio. If you prefer to use Python, see the Python Workshop for details."
  },
  {
    "objectID": "rdocs/r00-workshop.html#how-to-follow-the-workshops",
    "href": "rdocs/r00-workshop.html#how-to-follow-the-workshops",
    "title": "R Workshop",
    "section": "How to follow the workshops",
    "text": "How to follow the workshops\n\n Demo\nWhen you see this icon , the content will be demonstrated to you by the instructor. You should follow along and run the code in your own environment as you go.\n\n\n Exercises\nThroughout the workshop, you will be asked to complete some exercises. These will be marked with this icon . These tasks are designed to help you practice what you have learned. They should take no more than 5 minutes each."
  },
  {
    "objectID": "rdocs/r00-workshop.html#join-us",
    "href": "rdocs/r00-workshop.html#join-us",
    "title": "R Workshop",
    "section": "Join us",
    "text": "Join us\n\n\n\n\n\n\nWorkshop information\n\n\n\nThe next AgReFed Data-Harvester Workshop in R will be run on Tue, 25 Oct, 2022, at 09:30am AEST (Sydney).\n\nWorkshop starts in…\n Note: The above workshop is open to staff and students of The University of Sydney only. The next workshop, also in R, can be registered by anyone and is now open for registration. This workshop will be run on Thu, 03 Nov, 2022, at 01:30pm AEST (Sydney).\n\n\n\n Zoom link: Register online and link to the workshop will be sent, via email, closer to the workshop date.\n Workshop material: Click here to download the workshop material. (Link will be available closer to the workshop date.)"
  },
  {
    "objectID": "rdocs/r02-introduction.html",
    "href": "rdocs/r02-introduction.html",
    "title": " Introduction",
    "section": "",
    "text": "In this workshop\n\n\n\n\nLearn about AgReFed and the dataharvester package\nUse dataharvester to access and analyse data with a YAML configuration file\nPreview and manipulate downloaded data in GeoTIFF and .csv formats"
  },
  {
    "objectID": "rdocs/r02-introduction.html#introduction",
    "href": "rdocs/r02-introduction.html#introduction",
    "title": " Introduction",
    "section": "Introduction",
    "text": "Introduction\nHello and welcome to Workshop 1. In this workshop, we will learn about AgReFed and the dataharvester package and access data with a YAML configuration file.\n\n\n\n\n\n\n\nNote\n\n\n\nWe highly recommend you to run the workshop in RStudio Cloud. Make sure that you have looked at Setting up R as you must authenticate to Google Earth Engine (GEE) to access some data."
  },
  {
    "objectID": "rdocs/r02-introduction.html#fa-person-chalkboard-demo-1-authentication",
    "href": "rdocs/r02-introduction.html#fa-person-chalkboard-demo-1-authentication",
    "title": " Introduction",
    "section": " Demo 1: Authentication",
    "text": "Demo 1: Authentication\n\n\n\n\n\n\nAgenda\n\n\n\nIn this demo, we will:\n\ncheck that your reticulate environment is set up correctly;\nintialise Google Earth Engine (GEE), and authenticate to a Google account; and\nperform a simple test to validate that the Data-Harvester is working.\n\n\n\nRun the following code in your RStudio session:\nlibrary(dataharvester)\ninitialise_harvester(\"r-reticulate\")\nThe function will verify that the environment “r-reticulate” is available and contains all the required dependencies. If it does not, it will install them for you.\nOnce this is complete, you may want to authenticate to Google Earth Engine (GEE). This is not required for all data products, but it is required for some. To do this, add the following argument to the same code:\ninitialise_harvester(\"r-reticulate\", earthengine = TRUE)\nThe function will now try to authenticate to Google Earth Engine. This might involve opening a browser window and copying a code into the console.\nYou must initialise and authenticate to Google Earth Engine as each authentication key is tied to a specific Google account. Detailed instructions on authentication can also be found on the dataharvester package documentationon online."
  },
  {
    "objectID": "rdocs/r02-introduction.html#fa-person-chalkboard-demo-2-yaml-config",
    "href": "rdocs/r02-introduction.html#fa-person-chalkboard-demo-2-yaml-config",
    "title": " Introduction",
    "section": " Demo 2: YAML config",
    "text": "Demo 2: YAML config\n\n\n\n\n\n\nAgenda\n\n\n\nTo warm-up, we will:\n\nrun a simple YAML configuration file to demonstrate how it works;\npreview the output data; and\nlearn more about different configuration options.\n\n\n\nThe Data-Harvester uses a YAML configuration file to perform bulk downloads in a single command.\n\n\n\n\n\n\nWhat is YAML?\n\n\n\n\n\nYAML is a human-readable data-serialization language. It is commonly used for configuration files and in applications where data is being stored or transmitted. YAML files are easy to read and write, and are often used to configure software applications. A typical YAML file looks like this:\n---\nname: \"John Smith\"\nage: 42\noccupation: \"gardener\"\nLooks familiar? If you have used Markdown or R Markdown, YAML is used to define the metadata for the document.\nA YAML configuration file helps to keep track of the data you have downloaded and the analyses you have performed. It also allows you to easily reproduce your analyses and share them with others.\n\n\n\n\nBasic usage\nBelow is the YAML file for basic_config.yaml which can be found in the assets folder in RStudio Cloud. The file can also be downloaded from the GitHub repository.\n---\noutpath: downloads/\ntarget_bbox: [149.769345, -30.335861, 149.949173, -30.206271]\ntarget_dates: [2021]\ntarget_res: 6.0\ntarget_sources:\n  DEA: [landsat_barest_earth]\nTo run the configuration file, we use the harvest() function. The function takes a single argument, path_to_config, which is the path to the YAML file. The path can be either relative or absolute.\n\n\n\n\n\n\nRelative and absolute path strings\n\n\n\n\n\nA relative path is a path that is relative to the current working directory. For example, if you are working in the dataharvester directory, the path to the basic_config.yaml file, stored in the assets folder, is assets/basic_config.yaml.\nAn absolute path is a path that starts from the root directory. For example, the absolute path to the basic_config.yaml file is /home/rstudio/dataharvester/assets/basic_config.yaml.\nRelative paths are useful when you are working in a project directory, as they are shorter and easier to remember. Relative paths also allow you to share your code with others, as they will not need to change the path in the configuration file as long as they are working in the same folder. Absolute paths are useful when you are working in a different directory, or when you are working on a local machine.\n\n\n\nharvest(\"assets/basic_config.yaml\")\nSeveral optional arguments are available to customise the output of harvest():\n\nlog_name: the name of the log file output which stores some information about the download. The default is \"download_log.txt\".\npreview: if TRUE, the function will preview the output raster data\ncontour: if TRUE, the function will add a contour plot to the output raster data\n\nharvest(\"assets/basic_config.yaml\", \n  log_name = \"basic_config\", \n  preview = TRUE, \n  contour = TRUE)\nNote how the function recognises that a file has been downloaded and will not re-download it. This is useful if you want to re-run the configuration file to add more data sources – we will see this in action later.\n\n\nAdding multiple data sources\nAdditional data sources can be added to the configuration file by adding a new key to the target_sources list. Below we have added sources from DEM, Landscape, SILO and SLGA collections. Copy the new lines and add them to basic_config.yaml:\n---\noutpath: downloads/\ntarget_bbox: [149.769345, -30.335861, 149.949173, -30.206271]\ntarget_dates: [2021]\ntarget_res: 6.0\n\ntarget_sources:\n  DEA: [landsat_barest_earth]\n  # add the new lines below --------------------\n  DEM: [DEM]\n  Landscape: [Relief_300m]\n  SILO:\n    monthly_rain: [sum]\n  SLGA:\n    Bulk_Density: [0-5cm]\n    Clay: [0-5cm]\nLet’s run the above configuration file and preview the output data. You should already have the DEA data downloaded, so the function will only download the new data sources. In addition, the preview and contour arguments have been set to TRUE to preview the output data. We will also create a new log file, called \"multi_config.txt\" for this run.\nharvest(\"assets/multi_config.yaml\", \n  log_name = \"multi_config\", \n  preview = TRUE, \n  contour = TRUE)"
  },
  {
    "objectID": "rdocs/r02-introduction.html#fa-keyboard-exercise-1-update-config",
    "href": "rdocs/r02-introduction.html#fa-keyboard-exercise-1-update-config",
    "title": " Introduction",
    "section": " Exercise 1: Update config",
    "text": "Exercise 1: Update config\n\n\n\n\n\n\n On your own\n\n\n\nRefer to the YAML Overview section and update the basic_config.yaml file to download the following data sources:\n\nSlope and aspect from the Landscape collection\nTotal (sum) maximum temperature from the SILO collection\n\nThen, run harvest() on the configuration file and preview the output.\n\n\n\n\n\n\n\n\n Solution\n\n\n\n\n\n---\noutpath: downloads/\ntarget_bbox: [149.769345, -30.335861, 149.949173, -30.206271]\ntarget_dates: [2021]\ntarget_res: 6.0\n\ntarget_sources:\n  DEA: [landsat_barest_earth]\n  DEM: [DEM]\n  Landscape: [Relief_300m, Slope, Aspect]  # <- edit this line\n  SILO:\n    monthly_rain: [sum]\n    max_temp: [sum]  # <- add this line\n  SLGA:\n    Bulk_Density: [0-5cm]\n    Clay: [0-5cm]"
  },
  {
    "objectID": "rdocs/r02-introduction.html#fa-person-chalkboard-demo-3-google-earth-engine",
    "href": "rdocs/r02-introduction.html#fa-person-chalkboard-demo-3-google-earth-engine",
    "title": " Introduction",
    "section": " Demo 3: Google Earth Engine",
    "text": "Demo 3: Google Earth Engine\n\n\n\n\n\n\nAgenda\n\n\n\nTime to tap into the Google Earth Engine API. This demo will:\n\nIntroduce using the YAML configuration file to download data from Google Earth Engine\nDiscuss what can and cannot be done with the YAML configuration file\n\n\n\nWe will continue to use the same configuration file, basic_config.yaml, to download data from Google Earth Engine. Below, we add a new key to the target_sources list, called GEE. Workshop 2 will cover how to use the GEE API in more detail, but for now, we will preview how the YAML configuration file can be used download data from GEE.\n---\ninfile:  \noutpath: downloads/\ncolname_lat:  \ncolname_lng:  \ntarget_bbox: [149.769345, -30.335861, 149.949173, -30.206271]\ntarget_dates: [2021]\ntarget_res: 6.0\n\ntarget_sources:\n  DEA: [landsat_barest_earth]\n  DEM: [DEM]\n  Landscape: [Relief_300m, Slope, Aspect]  \n  SILO:\n    monthly_rain: [sum]\n    max_temp: [sum] \n  SLGA:\n    Bulk_Density: [0-5cm]\n    Clay: [0-5cm]\n  # add the new lines below --------------------\n  GEE:\n    preprocess:\n      collection: LANDSAT/LC09/C02/T1_L2\n      mask_clouds: True\n      reduce: median\n      spectral: [NDVI]\n    download:\n      bands: [NDVI, SR_B2, SR_B3, SR_B4]\n      scale: 100\nThe configuration will download a Landsat 9 images for the year 2021, composite them all to a single image using the median reducer, and calculate the NDVI spectral index. The output image will be downloaded as a GeoTIFF file, with the NDVI spectral index and the RGB bands (SR_B2, SR_B3, SR_B4) included in the raster image."
  },
  {
    "objectID": "rdocs/r02-introduction.html#fa-keyboard-exercise-2-gee-config",
    "href": "rdocs/r02-introduction.html#fa-keyboard-exercise-2-gee-config",
    "title": " Introduction",
    "section": " Exercise 2: GEE config",
    "text": "Exercise 2: GEE config\n\n\n\n\n\n\n On your own\n\n\n\nLet’s download from a different data source by referring to the Earth Engine Data Catalog. Have a look at the different sections and tabs (Description Bands, Image Properties, Terms of Use).\nTask\nChange the collection attribute in basic_config.yaml to one of Landsat, Sentinel or MODIS surface reflectance collections. Then, run harvest() on the configuration file and preview the output.\nQuestions\n\nWhat happens when you provide an incorrect dataset name?\nWhich other attribute(s) needed to be changed in the configuration file to download data from a different collection?\nWhat happens when you provide an incorrect band name?\n\n\n\nYour instructor will discuss this exercise in the next workshop."
  },
  {
    "objectID": "rdocs/r02-introduction.html#wrapping-up",
    "href": "rdocs/r02-introduction.html#wrapping-up",
    "title": " Introduction",
    "section": "Wrapping up",
    "text": "Wrapping up\nThis is the end of Workshop 1. In this workshop we have covered setup and using the YAML connfiguration file to download from multiple API sources. In the next workshop (Workshop 2), we will cover how to use individual download functions to create custom workflows for downloading data.\nSee you there!"
  },
  {
    "objectID": "rdocs/r04-advanced.html",
    "href": "rdocs/r04-advanced.html",
    "title": " Advanced features",
    "section": "",
    "text": "In this workshop\n\n\n\n\nDownloading large images from Google Earth Engine (5 min)\nTemporal Aggregation (10 min)\nPreviewing multiple images and timelapse (5 min)\nExploring other datasets in Earth Engine (5 min)\nSampling point data from rasters (10 min)\nExercise: Cloud and shadow masking (5 min)\nExercise: Spectral indices (5 min)"
  },
  {
    "objectID": "rdocs/r99-troubleshooting.html",
    "href": "rdocs/r99-troubleshooting.html",
    "title": " Troubleshooting",
    "section": "",
    "text": "Tip\n\n\n\nFound a bug? Open an issue on the dataharvester GitHub repository by clicking the “New Issue” button in the “Issues” tab.\n\n\n\nInstalling miniconda\nDuring installation of the dataharvester package, you may be asked to install r-miniconda. This is a package that allows you to install and use Python packages within R through the Conda binary package manager.\nIf you already have Python installed, R may still ask you to install r-miniconda. This is because r-miniconda is a separate package from Python, and is not the same as the Python package manager pip. Importantly, miniconda is almost necessary for Windows users as it allows you to install certain Python and non-Python packages without having to install a full version of Python and running installation commands in the command line. These include the GDAL binary which is a translator library for raster and vector geospatial data formats, and the PROJ library which is a cartographic projection and coordinate transformation library.\n\n\nActivating a Conda environment\n\n\nDependencies and GDAL\n\n\nGoogle Earth Engine authentication"
  }
]